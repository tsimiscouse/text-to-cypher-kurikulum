{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics Analysis (Ollama - Local LLM)\n",
    "\n",
    "Notebook ini menganalisis metrik evaluasi dari hasil agentic inference menggunakan **Ollama**.\n",
    "\n",
    "## Metrik yang dianalisis:\n",
    "- **String Metrics**: BLEU, Rouge-L, Jaro-Winkler, Jaccard Cypher\n",
    "- **Output Metrics**: Pass@1, Jaccard Output\n",
    "- **Composite Metrics**: LLMetric-Q, LLMetric\n",
    "- **Agentic Metrics**: Iterations, Recovery Rate, First Attempt Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/tsimiscouse/Docs/Sarjana/Skripsi/kg-luthfi\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "os.chdir(project_root)\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Ollama Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Ollama results found. Please run 01_agentic_inference_ollama.ipynb first.\n"
     ]
    }
   ],
   "source": [
    "# Load experiment summary from Ollama results\n",
    "results_dir = project_root / \"results_ollama\"\n",
    "summary_path = results_dir / \"experiment_summary.json\"\n",
    "\n",
    "if summary_path.exists():\n",
    "    with open(summary_path, \"r\") as f:\n",
    "        summary = json.load(f)\n",
    "    print(f\"Loaded results from: {summary['timestamp']}\")\n",
    "    print(f\"LLM Provider: {summary.get('llm_provider', 'ollama')}\")\n",
    "    print(f\"LLM Model: {summary.get('llm_model', 'qwen2.5-coder:3b')}\")\n",
    "    print(f\"Configurations: {list(summary['configurations'].keys())}\")\n",
    "else:\n",
    "    print(\"No Ollama results found. Please run 01_agentic_inference_ollama.ipynb first.\")\n",
    "    summary = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 results for Zero-Shot_Full\n",
      "Loaded 5 results for Zero-Shot_Nodes+Paths\n",
      "Loaded 5 results for Zero-Shot_Paths\n",
      "Loaded 5 results for Few-Shot_Full\n",
      "Loaded 5 results for Few-Shot_Nodes+Paths\n",
      "Loaded 5 results for Few-Shot_Paths\n",
      "Loaded 5 results for CoT_Full\n",
      "Loaded 5 results for CoT_Nodes+Paths\n",
      "Loaded 5 results for CoT_Paths\n"
     ]
    }
   ],
   "source": [
    "# Load detailed results for each configuration\n",
    "detailed_results = {}\n",
    "\n",
    "if summary:\n",
    "    for config_name in summary['configurations'].keys():\n",
    "        csv_path = results_dir / config_name / \"agentic_results.csv\"\n",
    "        if csv_path.exists():\n",
    "            detailed_results[config_name] = pd.read_csv(csv_path)\n",
    "            print(f\"Loaded {len(detailed_results[config_name])} results for {config_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "if summary:\n",
    "    rows = []\n",
    "    for name, config in summary['configurations'].items():\n",
    "        agentic = config.get('agentic_metrics', {})\n",
    "        rows.append({\n",
    "            \"Configuration\": name,\n",
    "            \"Prompt\": config['prompt_type'],\n",
    "            \"Schema\": config['schema_format'],\n",
    "            \"Pass@1 (%)\": round(config['pass_at_1_rate'], 2),\n",
    "            \"KG Valid (%)\": round(config['kg_valid_rate'], 2),\n",
    "            \"Avg BLEU\": round(config['avg_bleu'], 4),\n",
    "            \"Avg Rouge-L\": round(config['avg_rouge_l_f1'], 4),\n",
    "            \"Avg Jaro-Winkler\": round(config['avg_jaro_winkler'], 4),\n",
    "            \"Avg Jaccard Output\": round(config['avg_jaccard_output'], 4),\n",
    "            \"LLMetric-Q\": round(config['avg_llmetric_q'], 2),\n",
    "            \"LLMetric\": round(config['llmetric'], 2),\n",
    "            \"Avg Iterations\": round(agentic.get('avg_iterations', 0), 2),\n",
    "            \"Recovery Rate (%)\": round(agentic.get('recovery_rate', 0) * 100, 2),\n",
    "            \"First Attempt Success (%)\": round(agentic.get('first_attempt_success_rate', 0) * 100, 2),\n",
    "        })\n",
    "    \n",
    "    df_summary = pd.DataFrame(rows)\n",
    "    display(df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations Ranked by LLMetric:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Configuration</th>\n",
       "      <th>Pass@1 (%)</th>\n",
       "      <th>KG Valid (%)</th>\n",
       "      <th>LLMetric</th>\n",
       "      <th>Avg Iterations</th>\n",
       "      <th>Recovery Rate (%)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rank</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zero-Shot_Nodes+Paths</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>88.74</td>\n",
       "      <td>1.2</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zero-Shot_Paths</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>88.74</td>\n",
       "      <td>1.2</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Few-Shot_Full</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>88.69</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Few-Shot_Nodes+Paths</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>88.69</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Few-Shot_Paths</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>88.69</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CoT_Paths</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>88.69</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Zero-Shot_Full</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>88.64</td>\n",
       "      <td>1.2</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CoT_Full</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>88.57</td>\n",
       "      <td>1.2</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CoT_Nodes+Paths</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>88.57</td>\n",
       "      <td>1.2</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Configuration  Pass@1 (%)  KG Valid (%)  LLMetric  \\\n",
       "Rank                                                              \n",
       "1     Zero-Shot_Nodes+Paths        80.0         100.0     88.74   \n",
       "2           Zero-Shot_Paths        80.0         100.0     88.74   \n",
       "3             Few-Shot_Full        80.0         100.0     88.69   \n",
       "4      Few-Shot_Nodes+Paths        80.0         100.0     88.69   \n",
       "5            Few-Shot_Paths        80.0         100.0     88.69   \n",
       "6                 CoT_Paths        80.0         100.0     88.69   \n",
       "7            Zero-Shot_Full        80.0         100.0     88.64   \n",
       "8                  CoT_Full        80.0         100.0     88.57   \n",
       "9           CoT_Nodes+Paths        80.0         100.0     88.57   \n",
       "\n",
       "      Avg Iterations  Recovery Rate (%)  \n",
       "Rank                                     \n",
       "1                1.2              100.0  \n",
       "2                1.2              100.0  \n",
       "3                1.0                0.0  \n",
       "4                1.0                0.0  \n",
       "5                1.0                0.0  \n",
       "6                1.0                0.0  \n",
       "7                1.2              100.0  \n",
       "8                1.2              100.0  \n",
       "9                1.2              100.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rank configurations by LLMetric\n",
    "if summary:\n",
    "    df_ranked = df_summary.sort_values('LLMetric', ascending=False).reset_index(drop=True)\n",
    "    df_ranked.index = df_ranked.index + 1  # Start ranking from 1\n",
    "    df_ranked.index.name = 'Rank'\n",
    "    print(\"\\nConfigurations Ranked by LLMetric:\")\n",
    "    display(df_ranked[['Configuration', 'Pass@1 (%)', 'KG Valid (%)', 'LLMetric', 'Avg Iterations', 'Recovery Rate (%)']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass@1 Rate by Configuration\n",
    "if summary:\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    configs = list(df_summary['Configuration'])\n",
    "    pass_rates = df_summary['Pass@1 (%)']\n",
    "    \n",
    "    colors = plt.cm.Blues(np.linspace(0.4, 0.8, len(configs)))\n",
    "    bars = ax.bar(configs, pass_rates, color=colors)\n",
    "    \n",
    "    ax.set_xlabel('Configuration', fontsize=12)\n",
    "    ax.set_ylabel('Pass@1 Rate (%)', fontsize=12)\n",
    "    ax.set_title('Pass@1 Rate by Configuration (Ollama - Qwen2.5-Coder-3B)', fontsize=14)\n",
    "    ax.set_xticklabels(configs, rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, pass_rates):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{val:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'pass_at_1_by_config_ollama.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMetric Comparison\n",
    "if summary:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # LLMetric-Q Distribution\n",
    "    axes[0].bar(df_summary['Configuration'], df_summary['LLMetric-Q'], color='coral')\n",
    "    axes[0].set_xlabel('Configuration')\n",
    "    axes[0].set_ylabel('LLMetric-Q Score')\n",
    "    axes[0].set_title('Average LLMetric-Q by Configuration')\n",
    "    axes[0].set_xticklabels(df_summary['Configuration'], rotation=45, ha='right')\n",
    "    \n",
    "    # LLMetric\n",
    "    axes[1].bar(df_summary['Configuration'], df_summary['LLMetric'], color='seagreen')\n",
    "    axes[1].set_xlabel('Configuration')\n",
    "    axes[1].set_ylabel('LLMetric Score')\n",
    "    axes[1].set_title('LLMetric by Configuration')\n",
    "    axes[1].set_xticklabels(df_summary['Configuration'], rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'llmetric_comparison_ollama.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agentic Metrics: Iterations and Recovery\n",
    "if summary:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Average Iterations\n",
    "    axes[0].bar(df_summary['Configuration'], df_summary['Avg Iterations'], color='mediumpurple')\n",
    "    axes[0].set_xlabel('Configuration')\n",
    "    axes[0].set_ylabel('Average Iterations')\n",
    "    axes[0].set_title('Average Iterations Needed')\n",
    "    axes[0].set_xticklabels(df_summary['Configuration'], rotation=45, ha='right')\n",
    "    axes[0].axhline(y=1, color='red', linestyle='--', alpha=0.5, label='Single attempt')\n",
    "    \n",
    "    # Recovery Rate\n",
    "    axes[1].bar(df_summary['Configuration'], df_summary['Recovery Rate (%)'], color='darkorange')\n",
    "    axes[1].set_xlabel('Configuration')\n",
    "    axes[1].set_ylabel('Recovery Rate (%)')\n",
    "    axes[1].set_title('Recovery Rate (Initially Failed → Success)')\n",
    "    axes[1].set_xticklabels(df_summary['Configuration'], rotation=45, ha='right')\n",
    "    \n",
    "    # First Attempt Success\n",
    "    axes[2].bar(df_summary['Configuration'], df_summary['First Attempt Success (%)'], color='teal')\n",
    "    axes[2].set_xlabel('Configuration')\n",
    "    axes[2].set_ylabel('First Attempt Success (%)')\n",
    "    axes[2].set_title('First Attempt Success Rate')\n",
    "    axes[2].set_xticklabels(df_summary['Configuration'], rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'agentic_metrics_ollama.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis by Prompt Type and Schema Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by Prompt Type\n",
    "if summary:\n",
    "    prompt_agg = df_summary.groupby('Prompt').agg({\n",
    "        'Pass@1 (%)': 'mean',\n",
    "        'KG Valid (%)': 'mean',\n",
    "        'LLMetric': 'mean',\n",
    "        'Avg Iterations': 'mean',\n",
    "        'Recovery Rate (%)': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nAggregate by Prompt Type:\")\n",
    "    display(prompt_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by Schema Format\n",
    "if summary:\n",
    "    schema_agg = df_summary.groupby('Schema').agg({\n",
    "        'Pass@1 (%)': 'mean',\n",
    "        'KG Valid (%)': 'mean',\n",
    "        'LLMetric': 'mean',\n",
    "        'Avg Iterations': 'mean',\n",
    "        'Recovery Rate (%)': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nAggregate by Schema Format:\")\n",
    "    display(schema_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: Prompt vs Schema\n",
    "if summary:\n",
    "    # Create pivot table for LLMetric\n",
    "    pivot_llmetric = df_summary.pivot(index='Prompt', columns='Schema', values='LLMetric')\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # LLMetric Heatmap\n",
    "    sns.heatmap(pivot_llmetric, annot=True, fmt='.2f', cmap='YlGnBu', ax=axes[0])\n",
    "    axes[0].set_title('LLMetric: Prompt × Schema')\n",
    "    \n",
    "    # Pass@1 Heatmap\n",
    "    pivot_pass = df_summary.pivot(index='Prompt', columns='Schema', values='Pass@1 (%)')\n",
    "    sns.heatmap(pivot_pass, annot=True, fmt='.1f', cmap='YlGnBu', ax=axes[1])\n",
    "    axes[1].set_title('Pass@1 Rate (%): Prompt × Schema')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'heatmap_prompt_schema_ollama.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Per-Question Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis by Complexity Level (All Configurations):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Success Rate</th>\n",
       "      <th>Pass@1 Rate</th>\n",
       "      <th>Avg Iterations</th>\n",
       "      <th>Avg LLMetric-Q</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>complexity</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Easy</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.1111</td>\n",
       "      <td>90.0103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Success Rate  Pass@1 Rate  Avg Iterations  Avg LLMetric-Q\n",
       "complexity                                                           \n",
       "Easy                 1.0          0.8          1.1111         90.0103"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Analyze by complexity level\n",
    "if detailed_results:\n",
    "    # Combine all configurations\n",
    "    all_results = pd.concat([df.assign(config=name) for name, df in detailed_results.items()])\n",
    "    \n",
    "    complexity_stats = all_results.groupby('complexity').agg({\n",
    "        'success': 'mean',\n",
    "        'pass_at_1': 'mean',\n",
    "        'total_iterations': 'mean',\n",
    "        'llmetric_q': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    complexity_stats.columns = ['Success Rate', 'Pass@1 Rate', 'Avg Iterations', 'Avg LLMetric-Q']\n",
    "    print(\"\\nAnalysis by Complexity Level (All Configurations):\")\n",
    "    display(complexity_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze by reasoning level\n",
    "if detailed_results:\n",
    "    reasoning_stats = all_results.groupby('reasoning_level').agg({\n",
    "        'success': 'mean',\n",
    "        'pass_at_1': 'mean',\n",
    "        'total_iterations': 'mean',\n",
    "        'llmetric_q': 'mean'\n",
    "    }).round(4)\n",
    "    \n",
    "    reasoning_stats.columns = ['Success Rate', 'Pass@1 Rate', 'Avg Iterations', 'Avg LLMetric-Q']\n",
    "    print(\"\\nAnalysis by Reasoning Level (All Configurations):\")\n",
    "    display(reasoning_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error type distribution\n",
    "if summary:\n",
    "    print(\"\\n=== Error Type Distribution ===\")\n",
    "    \n",
    "    error_data = []\n",
    "    for config_name, config in summary['configurations'].items():\n",
    "        agentic = config.get('agentic_metrics', {})\n",
    "        error_dist = agentic.get('error_type_distribution', {})\n",
    "        recovery = agentic.get('error_recovery_by_type', {})\n",
    "        \n",
    "        for error_type, count in error_dist.items():\n",
    "            error_data.append({\n",
    "                'Configuration': config_name,\n",
    "                'Error Type': error_type,\n",
    "                'Count': count,\n",
    "                'Recovery Rate': recovery.get(error_type, 0) * 100\n",
    "            })\n",
    "    \n",
    "    if error_data:\n",
    "        df_errors = pd.DataFrame(error_data)\n",
    "        display(df_errors)\n",
    "        \n",
    "        # Visualize error distribution\n",
    "        fig, ax = plt.subplots(figsize=(12, 5))\n",
    "        \n",
    "        error_pivot = df_errors.pivot(index='Configuration', columns='Error Type', values='Count').fillna(0)\n",
    "        error_pivot.plot(kind='bar', ax=ax)\n",
    "        \n",
    "        ax.set_xlabel('Configuration')\n",
    "        ax.set_ylabel('Error Count')\n",
    "        ax.set_title('Error Type Distribution by Configuration')\n",
    "        ax.legend(title='Error Type')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(results_dir / 'error_distribution_ollama.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary to CSV\n",
    "if summary:\n",
    "    export_path = results_dir / 'metrics_summary_ollama.csv'\n",
    "    df_summary.to_csv(export_path, index=False)\n",
    "    print(f\"Exported summary to: {export_path}\")\n",
    "    \n",
    "    # Also export to Excel with multiple sheets\n",
    "    try:\n",
    "        with pd.ExcelWriter(results_dir / 'metrics_summary_ollama.xlsx', engine='openpyxl') as writer:\n",
    "            df_summary.to_excel(writer, sheet_name='Summary', index=False)\n",
    "            if 'prompt_agg' in dir():\n",
    "                prompt_agg.to_excel(writer, sheet_name='By Prompt')\n",
    "            if 'schema_agg' in dir():\n",
    "                schema_agg.to_excel(writer, sheet_name='By Schema')\n",
    "        print(f\"Exported Excel to: {results_dir / 'metrics_summary_ollama.xlsx'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not export to Excel: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
