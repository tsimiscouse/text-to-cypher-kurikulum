{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Analysis: Baseline (kg-axel) vs Agentic (Ollama)\n",
    "\n",
    "Notebook ini membandingkan hasil **Agentic Loop dengan Ollama** terhadap **Baseline dari kg-axel** (linear pipeline).\n",
    "\n",
    "## Perbandingan:\n",
    "- **Baseline**: Linear pipeline dari penelitian Axel (kg-axel)\n",
    "  - Model: GPT-4 / Qwen via Groq\n",
    "  - Approach: Single-pass generation\n",
    "  \n",
    "- **Agentic**: Self-correction loop dengan Ollama\n",
    "  - Model: Qwen2.5-Coder-3B (Local)\n",
    "  - Approach: Generate → Validate → Refine (max 5 iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "os.chdir(project_root)\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [14, 6]\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Baseline Results (from kg-axel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline results from Axel's research (kg-axel)\n",
    "# Source: combined_evaluation_metrics.xlsx (52 questions × 9 configurations)\n",
    "\n",
    "baseline_results = {\n",
    "    \"Zero-Shot_Full\": {\n",
    "        \"pass_at_1_rate\": 29.17,\n",
    "        \"kg_valid_rate\": 80.77,\n",
    "        \"llmetric\": 46.64,\n",
    "        \"avg_bleu\": 0.2842,\n",
    "        \"avg_rouge_l\": 0.6631,\n",
    "        \"avg_jaro\": 0.8946,\n",
    "    },\n",
    "    \"Zero-Shot_Nodes+Paths\": {\n",
    "        \"pass_at_1_rate\": 29.55,\n",
    "        \"kg_valid_rate\": 69.23,\n",
    "        \"llmetric\": 44.96,\n",
    "        \"avg_bleu\": 0.2614,\n",
    "        \"avg_rouge_l\": 0.6417,\n",
    "        \"avg_jaro\": 0.8765,\n",
    "    },\n",
    "    \"Zero-Shot_Paths\": {\n",
    "        \"pass_at_1_rate\": 29.79,\n",
    "        \"kg_valid_rate\": 78.85,\n",
    "        \"llmetric\": 47.16,\n",
    "        \"avg_bleu\": 0.2977,\n",
    "        \"avg_rouge_l\": 0.6753,\n",
    "        \"avg_jaro\": 0.8987,\n",
    "    },\n",
    "    \"Few-Shot_Full\": {\n",
    "        \"pass_at_1_rate\": 48.94,\n",
    "        \"kg_valid_rate\": 88.46,\n",
    "        \"llmetric\": 63.49,\n",
    "        \"avg_bleu\": 0.3156,\n",
    "        \"avg_rouge_l\": 0.7012,\n",
    "        \"avg_jaro\": 0.9123,\n",
    "    },\n",
    "    \"Few-Shot_Nodes+Paths\": {\n",
    "        \"pass_at_1_rate\": 44.19,\n",
    "        \"kg_valid_rate\": 78.85,\n",
    "        \"llmetric\": 58.74,\n",
    "        \"avg_bleu\": 0.2987,\n",
    "        \"avg_rouge_l\": 0.6854,\n",
    "        \"avg_jaro\": 0.9012,\n",
    "    },\n",
    "    \"Few-Shot_Paths\": {\n",
    "        \"pass_at_1_rate\": 47.92,\n",
    "        \"kg_valid_rate\": 86.54,\n",
    "        \"llmetric\": 62.73,\n",
    "        \"avg_bleu\": 0.3089,\n",
    "        \"avg_rouge_l\": 0.6945,\n",
    "        \"avg_jaro\": 0.9087,\n",
    "    },\n",
    "    \"CoT_Full\": {\n",
    "        \"pass_at_1_rate\": 44.00,\n",
    "        \"kg_valid_rate\": 90.38,\n",
    "        \"llmetric\": 60.51,\n",
    "        \"avg_bleu\": 0.2876,\n",
    "        \"avg_rouge_l\": 0.6789,\n",
    "        \"avg_jaro\": 0.8956,\n",
    "    },\n",
    "    \"CoT_Nodes+Paths\": {\n",
    "        \"pass_at_1_rate\": 39.22,\n",
    "        \"kg_valid_rate\": 96.15,\n",
    "        \"llmetric\": 59.24,\n",
    "        \"avg_bleu\": 0.2654,\n",
    "        \"avg_rouge_l\": 0.6543,\n",
    "        \"avg_jaro\": 0.8765,\n",
    "    },\n",
    "    \"CoT_Paths\": {\n",
    "        \"pass_at_1_rate\": 46.00,\n",
    "        \"kg_valid_rate\": 92.31,\n",
    "        \"llmetric\": 62.67,\n",
    "        \"avg_bleu\": 0.2945,\n",
    "        \"avg_rouge_l\": 0.6876,\n",
    "        \"avg_jaro\": 0.9012,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Baseline results from kg-axel loaded (52 questions per configuration)\")\n",
    "print(f\"Configurations: {list(baseline_results.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display baseline results as DataFrame\n",
    "df_baseline = pd.DataFrame(baseline_results).T\n",
    "df_baseline.index.name = 'Configuration'\n",
    "df_baseline = df_baseline.reset_index()\n",
    "\n",
    "print(\"\\nBaseline Results (kg-axel - Linear Pipeline):\")\n",
    "display(df_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Agentic Results (Ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load agentic results from Ollama\n",
    "results_dir = project_root / \"results_ollama\"\n",
    "summary_path = results_dir / \"experiment_summary.json\"\n",
    "\n",
    "if summary_path.exists():\n",
    "    with open(summary_path, \"r\") as f:\n",
    "        agentic_summary = json.load(f)\n",
    "    print(f\"Loaded agentic results from: {agentic_summary['timestamp']}\")\n",
    "    print(f\"LLM Provider: {agentic_summary.get('llm_provider', 'ollama')}\")\n",
    "    print(f\"LLM Model: {agentic_summary.get('llm_model', 'qwen2.5-coder:3b')}\")\n",
    "else:\n",
    "    print(\"No Ollama agentic results found.\")\n",
    "    print(\"Please run 01_agentic_inference_ollama.ipynb first.\")\n",
    "    agentic_summary = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs agentic\n",
    "if agentic_summary:\n",
    "    comparison_rows = []\n",
    "    \n",
    "    for config_name, agentic_config in agentic_summary['configurations'].items():\n",
    "        baseline = baseline_results.get(config_name, {})\n",
    "        \n",
    "        baseline_pass = baseline.get('pass_at_1_rate', 0)\n",
    "        baseline_kg = baseline.get('kg_valid_rate', 0)\n",
    "        baseline_llm = baseline.get('llmetric', 0)\n",
    "        \n",
    "        agentic_pass = agentic_config['pass_at_1_rate']\n",
    "        agentic_kg = agentic_config['kg_valid_rate']\n",
    "        agentic_llm = agentic_config['llmetric']\n",
    "        \n",
    "        agentic_metrics = agentic_config.get('agentic_metrics', {})\n",
    "        \n",
    "        # Calculate improvements\n",
    "        pass_delta = agentic_pass - baseline_pass\n",
    "        kg_delta = agentic_kg - baseline_kg\n",
    "        llm_delta = agentic_llm - baseline_llm\n",
    "        \n",
    "        # Calculate relative improvement (%)\n",
    "        pass_rel_imp = (pass_delta / baseline_pass * 100) if baseline_pass > 0 else 0\n",
    "        \n",
    "        comparison_rows.append({\n",
    "            \"Configuration\": config_name,\n",
    "            \"Baseline Pass@1 (%)\": round(baseline_pass, 2),\n",
    "            \"Agentic Pass@1 (%)\": round(agentic_pass, 2),\n",
    "            \"Pass@1 Δ (pp)\": round(pass_delta, 2),\n",
    "            \"Pass@1 Rel. Imp. (%)\": round(pass_rel_imp, 1),\n",
    "            \"Baseline KG Valid (%)\": round(baseline_kg, 2),\n",
    "            \"Agentic KG Valid (%)\": round(agentic_kg, 2),\n",
    "            \"KG Valid Δ (pp)\": round(kg_delta, 2),\n",
    "            \"Baseline LLMetric\": round(baseline_llm, 2),\n",
    "            \"Agentic LLMetric\": round(agentic_llm, 2),\n",
    "            \"LLMetric Δ\": round(llm_delta, 2),\n",
    "            \"Recovery Rate (%)\": round(agentic_metrics.get('recovery_rate', 0) * 100, 2),\n",
    "            \"Avg Iterations\": round(agentic_metrics.get('avg_iterations', 0), 2),\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_rows)\n",
    "    \n",
    "    print(\"\\nComparison: Baseline (kg-axel) vs Agentic (Ollama)\")\n",
    "    display(df_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics of improvements\n",
    "if agentic_summary:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"IMPROVEMENT SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    pass_improvements = df_comparison['Pass@1 Δ (pp)']\n",
    "    kg_improvements = df_comparison['KG Valid Δ (pp)']\n",
    "    llm_improvements = df_comparison['LLMetric Δ']\n",
    "    \n",
    "    print(f\"\\nPass@1 Rate Improvement:\")\n",
    "    print(f\"  Average: {pass_improvements.mean():+.2f} pp\")\n",
    "    print(f\"  Best: {pass_improvements.max():+.2f} pp ({df_comparison.loc[pass_improvements.idxmax(), 'Configuration']})\")\n",
    "    print(f\"  Worst: {pass_improvements.min():+.2f} pp ({df_comparison.loc[pass_improvements.idxmin(), 'Configuration']})\")\n",
    "    print(f\"  Configs Improved: {(pass_improvements > 0).sum()}/9\")\n",
    "    \n",
    "    print(f\"\\nKG Valid Rate Improvement:\")\n",
    "    print(f\"  Average: {kg_improvements.mean():+.2f} pp\")\n",
    "    print(f\"  Best: {kg_improvements.max():+.2f} pp\")\n",
    "    print(f\"  Configs Improved: {(kg_improvements > 0).sum()}/9\")\n",
    "    \n",
    "    print(f\"\\nLLMetric Improvement:\")\n",
    "    print(f\"  Average: {llm_improvements.mean():+.2f}\")\n",
    "    print(f\"  Best: {llm_improvements.max():+.2f} ({df_comparison.loc[llm_improvements.idxmax(), 'Configuration']})\")\n",
    "    print(f\"  Configs Improved: {(llm_improvements > 0).sum()}/9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass@1 Comparison: Baseline vs Agentic\n",
    "if agentic_summary:\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    x = np.arange(len(df_comparison))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, df_comparison['Baseline Pass@1 (%)'], width, \n",
    "                   label='Baseline (kg-axel)', color='gray', alpha=0.7)\n",
    "    bars2 = ax.bar(x + width/2, df_comparison['Agentic Pass@1 (%)'], width,\n",
    "                   label='Agentic (Ollama)', color='steelblue')\n",
    "    \n",
    "    ax.set_xlabel('Configuration', fontsize=12)\n",
    "    ax.set_ylabel('Pass@1 Rate (%)', fontsize=12)\n",
    "    ax.set_title('Pass@1 Rate: Baseline (Linear) vs Agentic (Self-Correction)', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df_comparison['Configuration'], rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'baseline_vs_agentic_pass1.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMetric Comparison\n",
    "if agentic_summary:\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    x = np.arange(len(df_comparison))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, df_comparison['Baseline LLMetric'], width, \n",
    "                   label='Baseline (kg-axel)', color='gray', alpha=0.7)\n",
    "    bars2 = ax.bar(x + width/2, df_comparison['Agentic LLMetric'], width,\n",
    "                   label='Agentic (Ollama)', color='seagreen')\n",
    "    \n",
    "    ax.set_xlabel('Configuration', fontsize=12)\n",
    "    ax.set_ylabel('LLMetric Score', fontsize=12)\n",
    "    ax.set_title('LLMetric: Baseline vs Agentic', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df_comparison['Configuration'], rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'baseline_vs_agentic_llmetric.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improvement Delta Visualization\n",
    "if agentic_summary:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    \n",
    "    # Pass@1 Delta\n",
    "    colors = ['green' if x > 0 else 'red' for x in df_comparison['Pass@1 Δ (pp)']]\n",
    "    axes[0].bar(df_comparison['Configuration'], df_comparison['Pass@1 Δ (pp)'], color=colors)\n",
    "    axes[0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    axes[0].set_xlabel('Configuration')\n",
    "    axes[0].set_ylabel('Improvement (pp)')\n",
    "    axes[0].set_title('Pass@1 Improvement (Agentic - Baseline)')\n",
    "    axes[0].set_xticklabels(df_comparison['Configuration'], rotation=45, ha='right')\n",
    "    \n",
    "    # KG Valid Delta\n",
    "    colors = ['green' if x > 0 else 'red' for x in df_comparison['KG Valid Δ (pp)']]\n",
    "    axes[1].bar(df_comparison['Configuration'], df_comparison['KG Valid Δ (pp)'], color=colors)\n",
    "    axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    axes[1].set_xlabel('Configuration')\n",
    "    axes[1].set_ylabel('Improvement (pp)')\n",
    "    axes[1].set_title('KG Validity Improvement')\n",
    "    axes[1].set_xticklabels(df_comparison['Configuration'], rotation=45, ha='right')\n",
    "    \n",
    "    # LLMetric Delta\n",
    "    colors = ['green' if x > 0 else 'red' for x in df_comparison['LLMetric Δ']]\n",
    "    axes[2].bar(df_comparison['Configuration'], df_comparison['LLMetric Δ'], color=colors)\n",
    "    axes[2].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    axes[2].set_xlabel('Configuration')\n",
    "    axes[2].set_ylabel('Improvement')\n",
    "    axes[2].set_title('LLMetric Improvement')\n",
    "    axes[2].set_xticklabels(df_comparison['Configuration'], rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'improvement_delta.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Self-Correction Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze where self-correction helped\n",
    "if agentic_summary:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SELF-CORRECTION IMPACT ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nQuestions that were initially wrong but corrected after refinement:\")\n",
    "    \n",
    "    correction_data = []\n",
    "    for config_name, config in agentic_summary['configurations'].items():\n",
    "        agentic_metrics = config.get('agentic_metrics', {})\n",
    "        \n",
    "        first_attempt_rate = agentic_metrics.get('first_attempt_success_rate', 0) * 100\n",
    "        final_success_rate = config['kg_valid_rate']\n",
    "        recovery_rate = agentic_metrics.get('recovery_rate', 0) * 100\n",
    "        \n",
    "        improvement_from_correction = final_success_rate - first_attempt_rate\n",
    "        \n",
    "        correction_data.append({\n",
    "            'Configuration': config_name,\n",
    "            'First Attempt Success (%)': round(first_attempt_rate, 1),\n",
    "            'Final Success (%)': round(final_success_rate, 1),\n",
    "            'Improvement from Correction (pp)': round(improvement_from_correction, 1),\n",
    "            'Recovery Rate (%)': round(recovery_rate, 1),\n",
    "            'Avg Iterations': round(agentic_metrics.get('avg_iterations', 0), 2)\n",
    "        })\n",
    "    \n",
    "    df_correction = pd.DataFrame(correction_data)\n",
    "    display(df_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize self-correction impact\n",
    "if agentic_summary:\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    x = np.arange(len(df_correction))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, df_correction['First Attempt Success (%)'], width, \n",
    "                   label='First Attempt', color='lightcoral')\n",
    "    bars2 = ax.bar(x + width/2, df_correction['Final Success (%)'], width,\n",
    "                   label='After Self-Correction', color='forestgreen')\n",
    "    \n",
    "    ax.set_xlabel('Configuration', fontsize=12)\n",
    "    ax.set_ylabel('Success Rate (%)', fontsize=12)\n",
    "    ax.set_title('Self-Correction Impact: First Attempt vs Final', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df_correction['Configuration'], rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 110)\n",
    "    \n",
    "    # Add improvement labels\n",
    "    for i, (idx, row) in enumerate(df_correction.iterrows()):\n",
    "        imp = row['Improvement from Correction (pp)']\n",
    "        if imp > 0:\n",
    "            ax.annotate(f'+{imp:.0f}pp', \n",
    "                       xy=(i + width/2, row['Final Success (%)']),\n",
    "                       ha='center', va='bottom', fontsize=9, color='green')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'self_correction_impact.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterations to Success Distribution\n",
    "if agentic_summary:\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (config_name, config) in enumerate(agentic_summary['configurations'].items()):\n",
    "        agentic_metrics = config.get('agentic_metrics', {})\n",
    "        iter_to_success = agentic_metrics.get('iterations_to_success', {})\n",
    "        \n",
    "        if iter_to_success:\n",
    "            iterations = [int(k) for k in iter_to_success.keys()]\n",
    "            counts = list(iter_to_success.values())\n",
    "            \n",
    "            colors = ['forestgreen' if it == 1 else 'orange' if it == 2 else 'salmon' \n",
    "                     for it in iterations]\n",
    "            axes[i].bar([str(x) for x in iterations], counts, color=colors)\n",
    "            axes[i].set_xlabel('Iterations Needed')\n",
    "            axes[i].set_ylabel('Count')\n",
    "            axes[i].set_title(config_name, fontsize=10)\n",
    "        else:\n",
    "            axes[i].text(0.5, 0.5, 'No data', ha='center', va='center')\n",
    "            axes[i].set_title(config_name, fontsize=10)\n",
    "    \n",
    "    plt.suptitle('Iterations to Success Distribution by Configuration', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'iterations_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall experiment statistics\n",
    "if agentic_summary:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OVERALL EXPERIMENT STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n--- BASELINE (kg-axel) ---\")\n",
    "    baseline_pass1 = [v['pass_at_1_rate'] for v in baseline_results.values()]\n",
    "    baseline_kg = [v['kg_valid_rate'] for v in baseline_results.values()]\n",
    "    baseline_llm = [v['llmetric'] for v in baseline_results.values()]\n",
    "    \n",
    "    print(f\"Pass@1: Mean={np.mean(baseline_pass1):.2f}%, Best={max(baseline_pass1):.2f}%\")\n",
    "    print(f\"KG Valid: Mean={np.mean(baseline_kg):.2f}%, Best={max(baseline_kg):.2f}%\")\n",
    "    print(f\"LLMetric: Mean={np.mean(baseline_llm):.2f}, Best={max(baseline_llm):.2f}\")\n",
    "    \n",
    "    print(\"\\n--- AGENTIC (Ollama) ---\")\n",
    "    agentic_pass1 = [c['pass_at_1_rate'] for c in agentic_summary['configurations'].values()]\n",
    "    agentic_kg = [c['kg_valid_rate'] for c in agentic_summary['configurations'].values()]\n",
    "    agentic_llm = [c['llmetric'] for c in agentic_summary['configurations'].values()]\n",
    "    \n",
    "    print(f\"Pass@1: Mean={np.mean(agentic_pass1):.2f}%, Best={max(agentic_pass1):.2f}%\")\n",
    "    print(f\"KG Valid: Mean={np.mean(agentic_kg):.2f}%, Best={max(agentic_kg):.2f}%\")\n",
    "    print(f\"LLMetric: Mean={np.mean(agentic_llm):.2f}, Best={max(agentic_llm):.2f}\")\n",
    "    \n",
    "    print(\"\\n--- IMPROVEMENT ---\")\n",
    "    print(f\"Pass@1: {np.mean(agentic_pass1) - np.mean(baseline_pass1):+.2f} pp\")\n",
    "    print(f\"KG Valid: {np.mean(agentic_kg) - np.mean(baseline_kg):+.2f} pp\")\n",
    "    print(f\"LLMetric: {np.mean(agentic_llm) - np.mean(baseline_llm):+.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best configurations\n",
    "if agentic_summary:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BEST CONFIGURATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Baseline best\n",
    "    best_baseline_pass1 = max(baseline_results.items(), key=lambda x: x[1]['pass_at_1_rate'])\n",
    "    best_baseline_llm = max(baseline_results.items(), key=lambda x: x[1]['llmetric'])\n",
    "    \n",
    "    print(f\"\\nBaseline Best Pass@1: {best_baseline_pass1[0]} ({best_baseline_pass1[1]['pass_at_1_rate']:.2f}%)\")\n",
    "    print(f\"Baseline Best LLMetric: {best_baseline_llm[0]} ({best_baseline_llm[1]['llmetric']:.2f})\")\n",
    "    \n",
    "    # Agentic best\n",
    "    best_agentic_pass1 = max(agentic_summary['configurations'].items(), \n",
    "                            key=lambda x: x[1]['pass_at_1_rate'])\n",
    "    best_agentic_llm = max(agentic_summary['configurations'].items(), \n",
    "                          key=lambda x: x[1]['llmetric'])\n",
    "    \n",
    "    print(f\"\\nAgentic Best Pass@1: {best_agentic_pass1[0]} ({best_agentic_pass1[1]['pass_at_1_rate']:.2f}%)\")\n",
    "    print(f\"Agentic Best LLMetric: {best_agentic_llm[0]} ({best_agentic_llm[1]['llmetric']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export comparison to CSV and Excel\n",
    "if agentic_summary:\n",
    "    comparative_dir = results_dir / \"comparative\"\n",
    "    comparative_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Export main comparison\n",
    "    export_path = comparative_dir / \"baseline_vs_agentic_ollama.csv\"\n",
    "    df_comparison.to_csv(export_path, index=False)\n",
    "    print(f\"Exported comparison to: {export_path}\")\n",
    "    \n",
    "    # Export self-correction analysis\n",
    "    correction_path = comparative_dir / \"self_correction_analysis.csv\"\n",
    "    df_correction.to_csv(correction_path, index=False)\n",
    "    print(f\"Exported correction analysis to: {correction_path}\")\n",
    "    \n",
    "    # Export to Excel with multiple sheets\n",
    "    try:\n",
    "        with pd.ExcelWriter(comparative_dir / 'comparative_analysis_ollama.xlsx', engine='openpyxl') as writer:\n",
    "            df_comparison.to_excel(writer, sheet_name='Comparison', index=False)\n",
    "            df_correction.to_excel(writer, sheet_name='Self-Correction', index=False)\n",
    "            df_baseline.to_excel(writer, sheet_name='Baseline', index=False)\n",
    "        print(f\"Exported Excel to: {comparative_dir / 'comparative_analysis_ollama.xlsx'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not export to Excel: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate conclusion\n",
    "if agentic_summary:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONCLUSION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    avg_pass_imp = np.mean(df_comparison['Pass@1 Δ (pp)'])\n",
    "    avg_llm_imp = np.mean(df_comparison['LLMetric Δ'])\n",
    "    configs_improved = (df_comparison['Pass@1 Δ (pp)'] > 0).sum()\n",
    "    \n",
    "    print(f\"\"\"\n",
    "Perbandingan antara Baseline (kg-axel) dan Agentic (Ollama) menunjukkan:\n",
    "\n",
    "1. PASS@1 RATE:\n",
    "   - Rata-rata improvement: {avg_pass_imp:+.2f} percentage points\n",
    "   - {configs_improved}/9 konfigurasi menunjukkan peningkatan\n",
    "\n",
    "2. LLMETRIC:\n",
    "   - Rata-rata improvement: {avg_llm_imp:+.2f}\n",
    "   \n",
    "3. SELF-CORRECTION EFFECTIVENESS:\n",
    "   - Recovery rate rata-rata: {df_correction['Recovery Rate (%)'].mean():.1f}%\n",
    "   - Rata-rata iterasi: {df_correction['Avg Iterations'].mean():.2f}\n",
    "\n",
    "4. CATATAN PENTING:\n",
    "   - Baseline menggunakan model lebih besar (GPT-4/Qwen-32B)\n",
    "   - Agentic menggunakan model lebih kecil (Qwen2.5-Coder-3B) + self-correction\n",
    "   - Self-correction dapat mengkompensasi keterbatasan model yang lebih kecil\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
