{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparative Analysis: Agentic vs Baseline (kg-axel)\n",
    "\n",
    "Notebook ini membandingkan hasil **Agentic Text2Cypher** dengan **Baseline** dari penelitian sebelumnya (kg-axel).\n",
    "\n",
    "## Comparison:\n",
    "- **Baseline**: kg-axel (single-pass inference, no self-correction)\n",
    "- **Agentic**: kg-luthfi (iterative refinement with validation feedback)\n",
    "\n",
    "## Key Questions:\n",
    "1. Does the agentic approach improve Pass@1 rate?\n",
    "2. How effective is self-correction (recovery rate)?\n",
    "3. Which configurations benefit most from agentic approach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "os.chdir(project_root)\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [14, 6]\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Baseline Results (kg-axel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline results from kg-axel research\n",
    "# Source: /Users/tsimiscouse/Docs/Sarjana/Skripsi/kg-axel/text2cypher/combined_evaluation_metrics.xlsx\n",
    "\n",
    "baseline_results = {\n",
    "    \"Zero-Shot_Full\": {\n",
    "        \"pass_at_1_rate\": 29.17,\n",
    "        \"kg_valid_rate\": 80.77,\n",
    "        \"avg_bleu\": 0.3636,\n",
    "        \"avg_rouge_l_f1\": 0.5568,\n",
    "        \"avg_jaro_winkler\": 0.7154,\n",
    "        \"avg_jaccard_output\": 0.3958,\n",
    "        \"avg_llmetric_q\": 40.74,\n",
    "        \"llmetric\": 46.64,\n",
    "    },\n",
    "    \"Zero-Shot_Nodes-Paths\": {\n",
    "        \"pass_at_1_rate\": 25.00,\n",
    "        \"kg_valid_rate\": 71.15,\n",
    "        \"avg_bleu\": 0.3229,\n",
    "        \"avg_rouge_l_f1\": 0.5290,\n",
    "        \"avg_jaro_winkler\": 0.6918,\n",
    "        \"avg_jaccard_output\": 0.3077,\n",
    "        \"avg_llmetric_q\": 36.33,\n",
    "        \"llmetric\": 40.24,\n",
    "    },\n",
    "    \"Zero-Shot_Only-Paths\": {\n",
    "        \"pass_at_1_rate\": 25.00,\n",
    "        \"kg_valid_rate\": 65.38,\n",
    "        \"avg_bleu\": 0.2942,\n",
    "        \"avg_rouge_l_f1\": 0.5053,\n",
    "        \"avg_jaro_winkler\": 0.6855,\n",
    "        \"avg_jaccard_output\": 0.3269,\n",
    "        \"avg_llmetric_q\": 35.68,\n",
    "        \"llmetric\": 38.24,\n",
    "    },\n",
    "    \"Few-Shot_Full\": {\n",
    "        \"pass_at_1_rate\": 48.94,\n",
    "        \"kg_valid_rate\": 88.46,\n",
    "        \"avg_bleu\": 0.4813,\n",
    "        \"avg_rouge_l_f1\": 0.6440,\n",
    "        \"avg_jaro_winkler\": 0.7754,\n",
    "        \"avg_jaccard_output\": 0.5532,\n",
    "        \"avg_llmetric_q\": 55.53,\n",
    "        \"llmetric\": 63.49,\n",
    "    },\n",
    "    \"Few-Shot_Nodes-Paths\": {\n",
    "        \"pass_at_1_rate\": 44.00,\n",
    "        \"kg_valid_rate\": 88.46,\n",
    "        \"avg_bleu\": 0.4592,\n",
    "        \"avg_rouge_l_f1\": 0.6167,\n",
    "        \"avg_jaro_winkler\": 0.7477,\n",
    "        \"avg_jaccard_output\": 0.5000,\n",
    "        \"avg_llmetric_q\": 52.81,\n",
    "        \"llmetric\": 60.51,\n",
    "    },\n",
    "    \"Few-Shot_Only-Paths\": {\n",
    "        \"pass_at_1_rate\": 45.83,\n",
    "        \"kg_valid_rate\": 87.50,\n",
    "        \"avg_bleu\": 0.4636,\n",
    "        \"avg_rouge_l_f1\": 0.6212,\n",
    "        \"avg_jaro_winkler\": 0.7580,\n",
    "        \"avg_jaccard_output\": 0.5161,\n",
    "        \"avg_llmetric_q\": 53.09,\n",
    "        \"llmetric\": 60.75,\n",
    "    },\n",
    "    \"CoT_Full\": {\n",
    "        \"pass_at_1_rate\": 44.00,\n",
    "        \"kg_valid_rate\": 90.38,\n",
    "        \"avg_bleu\": 0.4612,\n",
    "        \"avg_rouge_l_f1\": 0.6174,\n",
    "        \"avg_jaro_winkler\": 0.7546,\n",
    "        \"avg_jaccard_output\": 0.4898,\n",
    "        \"avg_llmetric_q\": 52.33,\n",
    "        \"llmetric\": 60.51,\n",
    "    },\n",
    "    \"CoT_Nodes-Paths\": {\n",
    "        \"pass_at_1_rate\": 42.31,\n",
    "        \"kg_valid_rate\": 88.46,\n",
    "        \"avg_bleu\": 0.4388,\n",
    "        \"avg_rouge_l_f1\": 0.5993,\n",
    "        \"avg_jaro_winkler\": 0.7397,\n",
    "        \"avg_jaccard_output\": 0.4681,\n",
    "        \"avg_llmetric_q\": 50.25,\n",
    "        \"llmetric\": 57.98,\n",
    "    },\n",
    "    \"CoT_Only-Paths\": {\n",
    "        \"pass_at_1_rate\": 44.23,\n",
    "        \"kg_valid_rate\": 86.54,\n",
    "        \"avg_bleu\": 0.4542,\n",
    "        \"avg_rouge_l_f1\": 0.6084,\n",
    "        \"avg_jaro_winkler\": 0.7463,\n",
    "        \"avg_jaccard_output\": 0.4894,\n",
    "        \"avg_llmetric_q\": 51.69,\n",
    "        \"llmetric\": 59.02,\n",
    "    },\n",
    "}\n",
    "\n",
    "df_baseline = pd.DataFrame([\n",
    "    {\"Configuration\": k, **v} for k, v in baseline_results.items()\n",
    "])\n",
    "\n",
    "print(\"Baseline Results (kg-axel):\")\n",
    "display(df_baseline[['Configuration', 'pass_at_1_rate', 'kg_valid_rate', 'llmetric']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Agentic Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load agentic results\n",
    "results_dir = project_root / \"results_v2\"\n",
    "metrics_path = results_dir / \"metrics_summary.csv\"\n",
    "\n",
    "if metrics_path.exists():\n",
    "    df_agentic = pd.read_csv(metrics_path)\n",
    "    print(f\"Loaded agentic results: {len(df_agentic)} configurations\")\n",
    "    display(df_agentic[['Configuration', 'Pass@1 (%)', 'KG Valid (%)', 'LLMetric']])\n",
    "else:\n",
    "    print(\"Agentic results not found.\")\n",
    "    print(\"Please run 02_evaluation_metrics.ipynb first.\")\n",
    "    df_agentic = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Comparison Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map configuration names (handle potential naming differences)\n",
    "config_mapping = {\n",
    "    \"Zero-Shot_Full\": \"Zero-Shot_Full\",\n",
    "    \"Zero-Shot_Nodes-Paths\": \"Zero-Shot_Nodes-Paths\",\n",
    "    \"Zero-Shot_Only-Paths\": \"Zero-Shot_Only-Paths\",\n",
    "    \"Few-Shot_Full\": \"Few-Shot_Full\",\n",
    "    \"Few-Shot_Nodes-Paths\": \"Few-Shot_Nodes-Paths\",\n",
    "    \"Few-Shot_Only-Paths\": \"Few-Shot_Only-Paths\",\n",
    "    \"CoT_Full\": \"CoT_Full\",\n",
    "    \"CoT_Nodes-Paths\": \"CoT_Nodes-Paths\",\n",
    "    \"CoT_Only-Paths\": \"CoT_Only-Paths\",\n",
    "}\n",
    "\n",
    "if df_agentic is not None:\n",
    "    # Normalize config names\n",
    "    df_agentic['Config_Normalized'] = df_agentic['Configuration'].map(\n",
    "        lambda x: config_mapping.get(x, x)\n",
    "    )\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    comparison_data = []\n",
    "    \n",
    "    for config_name in baseline_results.keys():\n",
    "        baseline = baseline_results[config_name]\n",
    "        \n",
    "        # Find matching agentic result\n",
    "        agentic_row = df_agentic[df_agentic['Config_Normalized'] == config_name]\n",
    "        \n",
    "        if len(agentic_row) > 0:\n",
    "            agentic = agentic_row.iloc[0]\n",
    "            \n",
    "            comparison_data.append({\n",
    "                \"Configuration\": config_name,\n",
    "                \"Baseline Pass@1\": baseline['pass_at_1_rate'],\n",
    "                \"Agentic Pass@1\": agentic['Pass@1 (%)'],\n",
    "                \"Pass@1 Delta\": agentic['Pass@1 (%)'] - baseline['pass_at_1_rate'],\n",
    "                \"Baseline KG Valid\": baseline['kg_valid_rate'],\n",
    "                \"Agentic KG Valid\": agentic['KG Valid (%)'],\n",
    "                \"KG Valid Delta\": agentic['KG Valid (%)'] - baseline['kg_valid_rate'],\n",
    "                \"Baseline LLMetric\": baseline['llmetric'],\n",
    "                \"Agentic LLMetric\": agentic['LLMetric'],\n",
    "                \"LLMetric Delta\": agentic['LLMetric'] - baseline['llmetric'],\n",
    "                \"Avg Iterations\": agentic['Avg Iterations'],\n",
    "                \"Recovery Rate\": agentic.get('Recovery Rate (%)', 0),\n",
    "            })\n",
    "        else:\n",
    "            comparison_data.append({\n",
    "                \"Configuration\": config_name,\n",
    "                \"Baseline Pass@1\": baseline['pass_at_1_rate'],\n",
    "                \"Agentic Pass@1\": None,\n",
    "                \"Pass@1 Delta\": None,\n",
    "                \"Baseline KG Valid\": baseline['kg_valid_rate'],\n",
    "                \"Agentic KG Valid\": None,\n",
    "                \"KG Valid Delta\": None,\n",
    "                \"Baseline LLMetric\": baseline['llmetric'],\n",
    "                \"Agentic LLMetric\": None,\n",
    "                \"LLMetric Delta\": None,\n",
    "                \"Avg Iterations\": None,\n",
    "                \"Recovery Rate\": None,\n",
    "            })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    print(\"\\nComparison: Baseline vs Agentic\")\n",
    "    display(df_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass@1 Comparison: Baseline vs Agentic\n",
    "if df_agentic is not None and len(df_comparison) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    x = np.arange(len(df_comparison))\n",
    "    width = 0.35\n",
    "    \n",
    "    baseline_vals = df_comparison['Baseline Pass@1'].values\n",
    "    agentic_vals = df_comparison['Agentic Pass@1'].fillna(0).values\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, baseline_vals, width, label='Baseline (kg-axel)', color='steelblue')\n",
    "    bars2 = ax.bar(x + width/2, agentic_vals, width, label='Agentic (kg-luthfi)', color='coral')\n",
    "    \n",
    "    ax.set_xlabel('Configuration', fontsize=12)\n",
    "    ax.set_ylabel('Pass@1 Rate (%)', fontsize=12)\n",
    "    ax.set_title('Pass@1 Rate: Baseline vs Agentic', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df_comparison['Configuration'], rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars1, baseline_vals):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{val:.1f}', ha='center', va='bottom', fontsize=8)\n",
    "    for bar, val in zip(bars2, agentic_vals):\n",
    "        if val > 0:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                    f'{val:.1f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'comparison_pass_at_1.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improvement Delta\n",
    "if df_agentic is not None and len(df_comparison) > 0:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Pass@1 Delta\n",
    "    deltas = df_comparison['Pass@1 Delta'].fillna(0).values\n",
    "    colors = ['green' if d > 0 else 'red' for d in deltas]\n",
    "    axes[0].bar(df_comparison['Configuration'], deltas, color=colors)\n",
    "    axes[0].set_xlabel('Configuration')\n",
    "    axes[0].set_ylabel('Delta (%)')\n",
    "    axes[0].set_title('Pass@1 Improvement (Agentic - Baseline)')\n",
    "    axes[0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    axes[0].set_xticklabels(df_comparison['Configuration'], rotation=45, ha='right')\n",
    "    \n",
    "    # KG Valid Delta\n",
    "    deltas = df_comparison['KG Valid Delta'].fillna(0).values\n",
    "    colors = ['green' if d > 0 else 'red' for d in deltas]\n",
    "    axes[1].bar(df_comparison['Configuration'], deltas, color=colors)\n",
    "    axes[1].set_xlabel('Configuration')\n",
    "    axes[1].set_ylabel('Delta (%)')\n",
    "    axes[1].set_title('KG Valid Improvement (Agentic - Baseline)')\n",
    "    axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    axes[1].set_xticklabels(df_comparison['Configuration'], rotation=45, ha='right')\n",
    "    \n",
    "    # LLMetric Delta\n",
    "    deltas = df_comparison['LLMetric Delta'].fillna(0).values\n",
    "    colors = ['green' if d > 0 else 'red' for d in deltas]\n",
    "    axes[2].bar(df_comparison['Configuration'], deltas, color=colors)\n",
    "    axes[2].set_xlabel('Configuration')\n",
    "    axes[2].set_ylabel('Delta')\n",
    "    axes[2].set_title('LLMetric Improvement (Agentic - Baseline)')\n",
    "    axes[2].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    axes[2].set_xticklabels(df_comparison['Configuration'], rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'comparison_improvement_delta.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Correction Impact\n",
    "if df_agentic is not None and len(df_comparison) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Recovery Rate vs Improvement\n",
    "    valid_data = df_comparison.dropna(subset=['Recovery Rate', 'Pass@1 Delta'])\n",
    "    \n",
    "    if len(valid_data) > 0:\n",
    "        axes[0].scatter(valid_data['Recovery Rate'], valid_data['Pass@1 Delta'], \n",
    "                       s=100, c='coral', alpha=0.7)\n",
    "        \n",
    "        for _, row in valid_data.iterrows():\n",
    "            axes[0].annotate(row['Configuration'].split('_')[0], \n",
    "                           (row['Recovery Rate'], row['Pass@1 Delta']),\n",
    "                           fontsize=8)\n",
    "        \n",
    "        axes[0].set_xlabel('Recovery Rate (%)')\n",
    "        axes[0].set_ylabel('Pass@1 Improvement (%)')\n",
    "        axes[0].set_title('Recovery Rate vs Pass@1 Improvement')\n",
    "        axes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Iterations vs Recovery Rate\n",
    "    valid_data = df_comparison.dropna(subset=['Avg Iterations', 'Recovery Rate'])\n",
    "    \n",
    "    if len(valid_data) > 0:\n",
    "        axes[1].scatter(valid_data['Avg Iterations'], valid_data['Recovery Rate'],\n",
    "                       s=100, c='mediumpurple', alpha=0.7)\n",
    "        \n",
    "        for _, row in valid_data.iterrows():\n",
    "            axes[1].annotate(row['Configuration'].split('_')[0],\n",
    "                           (row['Avg Iterations'], row['Recovery Rate']),\n",
    "                           fontsize=8)\n",
    "        \n",
    "        axes[1].set_xlabel('Average Iterations')\n",
    "        axes[1].set_ylabel('Recovery Rate (%)')\n",
    "        axes[1].set_title('Iterations vs Recovery Rate')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'comparison_self_correction.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Statistics\n",
    "if df_agentic is not None and len(df_comparison) > 0:\n",
    "    valid_comparison = df_comparison.dropna()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"OVERALL STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nConfigurations compared: {len(valid_comparison)}\")\n",
    "    \n",
    "    # Average improvements\n",
    "    avg_pass1_delta = valid_comparison['Pass@1 Delta'].mean()\n",
    "    avg_kg_valid_delta = valid_comparison['KG Valid Delta'].mean()\n",
    "    avg_llmetric_delta = valid_comparison['LLMetric Delta'].mean()\n",
    "    \n",
    "    print(f\"\\nAverage Pass@1 Improvement: {avg_pass1_delta:+.2f}%\")\n",
    "    print(f\"Average KG Valid Improvement: {avg_kg_valid_delta:+.2f}%\")\n",
    "    print(f\"Average LLMetric Improvement: {avg_llmetric_delta:+.2f}\")\n",
    "    \n",
    "    # Improvements count\n",
    "    improved_pass1 = (valid_comparison['Pass@1 Delta'] > 0).sum()\n",
    "    improved_llmetric = (valid_comparison['LLMetric Delta'] > 0).sum()\n",
    "    \n",
    "    print(f\"\\nConfigurations with improved Pass@1: {improved_pass1}/{len(valid_comparison)}\")\n",
    "    print(f\"Configurations with improved LLMetric: {improved_llmetric}/{len(valid_comparison)}\")\n",
    "    \n",
    "    # Best and worst improvements\n",
    "    best_improvement = valid_comparison.loc[valid_comparison['Pass@1 Delta'].idxmax()]\n",
    "    worst_improvement = valid_comparison.loc[valid_comparison['Pass@1 Delta'].idxmin()]\n",
    "    \n",
    "    print(f\"\\nBest Pass@1 improvement: {best_improvement['Configuration']} ({best_improvement['Pass@1 Delta']:+.2f}%)\")\n",
    "    print(f\"Worst Pass@1 change: {worst_improvement['Configuration']} ({worst_improvement['Pass@1 Delta']:+.2f}%)\")\n",
    "    \n",
    "    # Recovery effectiveness\n",
    "    avg_recovery = valid_comparison['Recovery Rate'].mean()\n",
    "    avg_iterations = valid_comparison['Avg Iterations'].mean()\n",
    "    \n",
    "    print(f\"\\nAverage Recovery Rate: {avg_recovery:.2f}%\")\n",
    "    print(f\"Average Iterations: {avg_iterations:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis by Prompt Type\n",
    "if df_agentic is not None and len(df_comparison) > 0:\n",
    "    valid_comparison = df_comparison.dropna()\n",
    "    valid_comparison['Prompt'] = valid_comparison['Configuration'].apply(lambda x: x.split('_')[0])\n",
    "    \n",
    "    prompt_stats = valid_comparison.groupby('Prompt').agg({\n",
    "        'Baseline Pass@1': 'mean',\n",
    "        'Agentic Pass@1': 'mean',\n",
    "        'Pass@1 Delta': 'mean',\n",
    "        'LLMetric Delta': 'mean',\n",
    "        'Recovery Rate': 'mean',\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANALYSIS BY PROMPT TYPE\")\n",
    "    print(\"=\"*60)\n",
    "    display(prompt_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate conclusions\n",
    "if df_agentic is not None and len(df_comparison) > 0:\n",
    "    valid_comparison = df_comparison.dropna()\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"CONCLUSIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    avg_pass1_delta = valid_comparison['Pass@1 Delta'].mean()\n",
    "    avg_recovery = valid_comparison['Recovery Rate'].mean()\n",
    "    improved_count = (valid_comparison['Pass@1 Delta'] > 0).sum()\n",
    "    total_count = len(valid_comparison)\n",
    "    \n",
    "    print(f\"\\n1. EFFECTIVENESS OF AGENTIC APPROACH:\")\n",
    "    if avg_pass1_delta > 0:\n",
    "        print(f\"   - Agentic approach IMPROVES Pass@1 rate by average of {avg_pass1_delta:.2f}%\")\n",
    "    else:\n",
    "        print(f\"   - Agentic approach shows mixed results (avg delta: {avg_pass1_delta:.2f}%)\")\n",
    "    \n",
    "    print(f\"   - {improved_count}/{total_count} configurations show improvement\")\n",
    "    \n",
    "    print(f\"\\n2. SELF-CORRECTION CAPABILITY:\")\n",
    "    print(f\"   - Average Recovery Rate: {avg_recovery:.2f}%\")\n",
    "    print(f\"   - Self-correction {'is effective' if avg_recovery > 30 else 'has limited effectiveness'}\")\n",
    "    \n",
    "    # Best performing prompt type\n",
    "    valid_comparison['Prompt'] = valid_comparison['Configuration'].apply(lambda x: x.split('_')[0])\n",
    "    best_prompt = valid_comparison.groupby('Prompt')['Pass@1 Delta'].mean().idxmax()\n",
    "    best_prompt_delta = valid_comparison.groupby('Prompt')['Pass@1 Delta'].mean().max()\n",
    "    \n",
    "    print(f\"\\n3. BEST PERFORMING PROMPT TYPE:\")\n",
    "    print(f\"   - {best_prompt} shows best improvement ({best_prompt_delta:+.2f}%)\")\n",
    "    \n",
    "    print(f\"\\n4. RECOMMENDATION:\")\n",
    "    if avg_pass1_delta > 5:\n",
    "        print(\"   - Agentic approach is RECOMMENDED for Text2Cypher tasks\")\n",
    "    elif avg_pass1_delta > 0:\n",
    "        print(\"   - Agentic approach provides MODEST improvements\")\n",
    "    else:\n",
    "        print(\"   - Consider optimizing self-correction strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export comparison results\n",
    "if df_agentic is not None and len(df_comparison) > 0:\n",
    "    # Save comparison CSV\n",
    "    df_comparison.to_csv(results_dir / 'comparative_analysis.csv', index=False)\n",
    "    print(f\"Comparison saved to: {results_dir / 'comparative_analysis.csv'}\")\n",
    "    \n",
    "    # Save to Excel\n",
    "    try:\n",
    "        with pd.ExcelWriter(results_dir / 'comparative_analysis.xlsx', engine='openpyxl') as writer:\n",
    "            df_comparison.to_excel(writer, sheet_name='Comparison', index=False)\n",
    "            df_baseline.to_excel(writer, sheet_name='Baseline', index=False)\n",
    "            if 'df_metrics' in dir():\n",
    "                df_agentic.to_excel(writer, sheet_name='Agentic', index=False)\n",
    "        print(f\"Excel saved to: {results_dir / 'comparative_analysis.xlsx'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not save Excel: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARATIVE ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nResults saved to: {results_dir}\")\n",
    "print(\"\\nFiles generated:\")\n",
    "print(\"  - comparative_analysis.csv\")\n",
    "print(\"  - comparative_analysis.xlsx\")\n",
    "print(\"  - comparison_*.png (visualizations)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
