{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch 2: Agentic Text2Cypher Inference (Day 2/5)\n",
    "\n",
    "**Configurations**: Zero-Shot_Only-Paths, Few-Shot_Full\n",
    "\n",
    "**Model**: Qwen3-32B via Groq API\n",
    "\n",
    "**Questions**: 52 Ã— 2 = 104 inferences\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "- Batch 1 completed (Zero-Shot_Full, Zero-Shot_Nodes+Paths)\n",
    "- Rate limit reset (new day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "os.chdir(project_root)\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from config.settings import Settings\n",
    "from config.llm_config import LLMConfig\n",
    "from data.ground_truth_loader import GroundTruthLoader\n",
    "from prompts.prompt_manager import PromptManager, PromptType, SchemaFormat\n",
    "from experiment.batch_processor import BatchProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "settings = Settings()\n",
    "llm_config = LLMConfig()\n",
    "llm_config.validate()\n",
    "\n",
    "print(f\"Model: {llm_config.model}\")\n",
    "print(f\"Max Iterations: {settings.max_iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth\n",
    "loader = GroundTruthLoader()\n",
    "items = loader.load()\n",
    "print(f\"Loaded {len(items)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 2 Configurations\n",
    "\n",
    "| # | Prompt Type | Schema Format | Config Name |\n",
    "|---|-------------|---------------|-------------|\n",
    "| 3 | Zero-Shot | Only-Paths | Zero-Shot_Only-Paths |\n",
    "| 4 | Few-Shot | Full Schema | Few-Shot_Full |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Batch 2 configurations\n",
    "BATCH_2_CONFIGS = [\n",
    "    {\"prompt\": PromptType.ZERO_SHOT, \"schema\": SchemaFormat.ONLY_PATHS},\n",
    "    {\"prompt\": PromptType.FEW_SHOT, \"schema\": SchemaFormat.FULL_SCHEMA},\n",
    "]\n",
    "\n",
    "print(f\"Batch 2: {len(BATCH_2_CONFIGS)} configurations\")\n",
    "print(f\"Total inferences: {len(BATCH_2_CONFIGS) * len(items)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup results directory\n",
    "results_dir = project_root / \"results_v2\"\n",
    "batch_results_dir = results_dir / \"batch_2\"\n",
    "batch_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Results will be saved to: {batch_results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor\n",
    "processor = BatchProcessor(\n",
    "    settings=settings,\n",
    "    llm_config=llm_config,\n",
    "    checkpoint_dir=str(batch_results_dir / \"checkpoints\")\n",
    ")\n",
    "\n",
    "prompt_manager = PromptManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Batch 2 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "batch_2_results = {}\n",
    "\n",
    "for i, config in enumerate(BATCH_2_CONFIGS):\n",
    "    config_name = prompt_manager.get_configuration_name(config[\"prompt\"], config[\"schema\"])\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Configuration {i+1}/{len(BATCH_2_CONFIGS)}: {config_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    config_dir = batch_results_dir / config_name\n",
    "    config_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        results = processor.process_batch(\n",
    "            items=items,\n",
    "            prompt_type=config[\"prompt\"],\n",
    "            schema_format=config[\"schema\"],\n",
    "            batch_id=f\"batch2_{config_name}\",\n",
    "            resume=True\n",
    "        )\n",
    "        \n",
    "        batch_2_results[config_name] = results\n",
    "        \n",
    "        # Save results\n",
    "        results_data = [state.to_dict() for state in results]\n",
    "        with open(config_dir / \"agentic_results.json\", \"w\") as f:\n",
    "            json.dump(results_data, f, indent=2, default=str)\n",
    "        \n",
    "        df = pd.DataFrame([{\n",
    "            \"question_id\": s.question_id,\n",
    "            \"question\": s.question,\n",
    "            \"ground_truth\": s.ground_truth_query,\n",
    "            \"final_query\": s.final_query,\n",
    "            \"success\": s.success,\n",
    "            \"pass_at_1\": s.first_attempt_success,\n",
    "            \"total_iterations\": s.total_iterations,\n",
    "            \"kg_valid\": s.kg_valid,\n",
    "        } for s in results])\n",
    "        df.to_csv(config_dir / \"agentic_results.csv\", index=False)\n",
    "        \n",
    "        # Stats\n",
    "        success_rate = sum(1 for s in results if s.success) / len(results) * 100\n",
    "        pass_at_1 = sum(1 for s in results if s.first_attempt_success) / len(results) * 100\n",
    "        avg_iterations = sum(s.total_iterations for s in results) / len(results)\n",
    "        \n",
    "        print(f\"\\n{config_name} Results:\")\n",
    "        print(f\"  Success Rate: {success_rate:.1f}%\")\n",
    "        print(f\"  Pass@1 Rate: {pass_at_1:.1f}%\")\n",
    "        print(f\"  Avg Iterations: {avg_iterations:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {config_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 2 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary\n",
    "if batch_2_results:\n",
    "    summary_rows = []\n",
    "    \n",
    "    for config_name, results in batch_2_results.items():\n",
    "        success_count = sum(1 for s in results if s.success)\n",
    "        pass_at_1_count = sum(1 for s in results if s.first_attempt_success)\n",
    "        kg_valid_count = sum(1 for s in results if s.kg_valid)\n",
    "        \n",
    "        summary_rows.append({\n",
    "            \"Configuration\": config_name,\n",
    "            \"Total\": len(results),\n",
    "            \"Success\": success_count,\n",
    "            \"Success Rate (%)\": round(success_count / len(results) * 100, 2),\n",
    "            \"Pass@1\": pass_at_1_count,\n",
    "            \"Pass@1 Rate (%)\": round(pass_at_1_count / len(results) * 100, 2),\n",
    "            \"KG Valid\": kg_valid_count,\n",
    "            \"KG Valid Rate (%)\": round(kg_valid_count / len(results) * 100, 2),\n",
    "            \"Avg Iterations\": round(sum(s.total_iterations for s in results) / len(results), 2),\n",
    "        })\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_rows)\n",
    "    display(df_summary)\n",
    "    \n",
    "    df_summary.to_csv(batch_results_dir / \"batch_2_summary.csv\", index=False)\n",
    "    print(f\"\\nSummary saved to: {batch_results_dir / 'batch_2_summary.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata\n",
    "batch_metadata = {\n",
    "    \"batch_number\": 2,\n",
    "    \"date\": datetime.now().isoformat(),\n",
    "    \"configurations\": [prompt_manager.get_configuration_name(c[\"prompt\"], c[\"schema\"]) for c in BATCH_2_CONFIGS],\n",
    "    \"total_questions\": len(items),\n",
    "    \"total_inferences\": len(BATCH_2_CONFIGS) * len(items),\n",
    "    \"llm_model\": llm_config.model,\n",
    "    \"max_iterations\": settings.max_iterations,\n",
    "}\n",
    "\n",
    "with open(batch_results_dir / \"batch_2_metadata.json\", \"w\") as f:\n",
    "    json.dump(batch_metadata, f, indent=2)\n",
    "\n",
    "print(\"Batch 2 completed!\")\n",
    "print(\"Continue with batch-3_01_agentic_inference.ipynb tomorrow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
