{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch 5: Agentic Text2Cypher Inference (Day 5/5) - FINAL\n",
    "\n",
    "**Configurations**: CoT_Only-Paths\n",
    "\n",
    "**Model**: Qwen3-32B via Groq API\n",
    "\n",
    "**Questions**: 52 Ã— 1 = 52 inferences\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "- Batch 1, 2, 3 & 4 completed\n",
    "- Rate limit reset (new day)\n",
    "\n",
    "## Note\n",
    "This is the final batch. After completion, run the consolidation cells to merge all results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "os.chdir(project_root)\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from config.settings import Settings\n",
    "from config.llm_config import LLMConfig\n",
    "from data.ground_truth_loader import GroundTruthLoader\n",
    "from prompts.prompt_manager import PromptManager, PromptType, SchemaFormat\n",
    "from experiment.batch_processor import BatchProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "settings = Settings()\n",
    "llm_config = LLMConfig()\n",
    "llm_config.validate()\n",
    "\n",
    "print(f\"Model: {llm_config.model}\")\n",
    "print(f\"Max Iterations: {settings.max_iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth\n",
    "loader = GroundTruthLoader()\n",
    "items = loader.load()\n",
    "print(f\"Loaded {len(items)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 5 Configuration (Final)\n",
    "\n",
    "| # | Prompt Type | Schema Format | Config Name |\n",
    "|---|-------------|---------------|-------------|\n",
    "| 9 | CoT | Only-Paths | CoT_Only-Paths |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Batch 5 configuration\n",
    "BATCH_5_CONFIGS = [\n",
    "    {\"prompt\": PromptType.CHAIN_OF_THOUGHT, \"schema\": SchemaFormat.ONLY_PATHS},\n",
    "]\n",
    "\n",
    "print(f\"Batch 5 (Final): {len(BATCH_5_CONFIGS)} configuration\")\n",
    "print(f\"Total inferences: {len(BATCH_5_CONFIGS) * len(items)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup results directory\n",
    "results_dir = project_root / \"results_v2\"\n",
    "batch_results_dir = results_dir / \"batch_5\"\n",
    "batch_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Results will be saved to: {batch_results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor\n",
    "processor = BatchProcessor(\n",
    "    settings=settings,\n",
    "    llm_config=llm_config,\n",
    "    checkpoint_dir=str(batch_results_dir / \"checkpoints\")\n",
    ")\n",
    "\n",
    "prompt_manager = PromptManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Batch 5 Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment\n",
    "batch_5_results = {}\n",
    "\n",
    "for i, config in enumerate(BATCH_5_CONFIGS):\n",
    "    config_name = prompt_manager.get_configuration_name(config[\"prompt\"], config[\"schema\"])\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Configuration: {config_name} (FINAL)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    config_dir = batch_results_dir / config_name\n",
    "    config_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        results = processor.process_batch(\n",
    "            items=items,\n",
    "            prompt_type=config[\"prompt\"],\n",
    "            schema_format=config[\"schema\"],\n",
    "            batch_id=f\"batch5_{config_name}\",\n",
    "            resume=True\n",
    "        )\n",
    "        \n",
    "        batch_5_results[config_name] = results\n",
    "        \n",
    "        # Save results\n",
    "        results_data = [state.to_dict() for state in results]\n",
    "        with open(config_dir / \"agentic_results.json\", \"w\") as f:\n",
    "            json.dump(results_data, f, indent=2, default=str)\n",
    "        \n",
    "        df = pd.DataFrame([{\n",
    "            \"question_id\": s.question_id,\n",
    "            \"question\": s.question,\n",
    "            \"ground_truth\": s.ground_truth_query,\n",
    "            \"final_query\": s.final_query,\n",
    "            \"success\": s.success,\n",
    "            \"pass_at_1\": s.first_attempt_success,\n",
    "            \"total_iterations\": s.total_iterations,\n",
    "            \"kg_valid\": s.kg_valid,\n",
    "        } for s in results])\n",
    "        df.to_csv(config_dir / \"agentic_results.csv\", index=False)\n",
    "        \n",
    "        # Stats\n",
    "        success_rate = sum(1 for s in results if s.success) / len(results) * 100\n",
    "        pass_at_1 = sum(1 for s in results if s.first_attempt_success) / len(results) * 100\n",
    "        avg_iterations = sum(s.total_iterations for s in results) / len(results)\n",
    "        \n",
    "        print(f\"\\n{config_name} Results:\")\n",
    "        print(f\"  Success Rate: {success_rate:.1f}%\")\n",
    "        print(f\"  Pass@1 Rate: {pass_at_1:.1f}%\")\n",
    "        print(f\"  Avg Iterations: {avg_iterations:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {config_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 5 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary\n",
    "if batch_5_results:\n",
    "    summary_rows = []\n",
    "    \n",
    "    for config_name, results in batch_5_results.items():\n",
    "        success_count = sum(1 for s in results if s.success)\n",
    "        pass_at_1_count = sum(1 for s in results if s.first_attempt_success)\n",
    "        kg_valid_count = sum(1 for s in results if s.kg_valid)\n",
    "        \n",
    "        summary_rows.append({\n",
    "            \"Configuration\": config_name,\n",
    "            \"Total\": len(results),\n",
    "            \"Success\": success_count,\n",
    "            \"Success Rate (%)\": round(success_count / len(results) * 100, 2),\n",
    "            \"Pass@1\": pass_at_1_count,\n",
    "            \"Pass@1 Rate (%)\": round(pass_at_1_count / len(results) * 100, 2),\n",
    "            \"KG Valid\": kg_valid_count,\n",
    "            \"KG Valid Rate (%)\": round(kg_valid_count / len(results) * 100, 2),\n",
    "            \"Avg Iterations\": round(sum(s.total_iterations for s in results) / len(results), 2),\n",
    "        })\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_rows)\n",
    "    display(df_summary)\n",
    "    \n",
    "    df_summary.to_csv(batch_results_dir / \"batch_5_summary.csv\", index=False)\n",
    "    print(f\"\\nSummary saved to: {batch_results_dir / 'batch_5_summary.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata\n",
    "batch_metadata = {\n",
    "    \"batch_number\": 5,\n",
    "    \"date\": datetime.now().isoformat(),\n",
    "    \"configurations\": [prompt_manager.get_configuration_name(c[\"prompt\"], c[\"schema\"]) for c in BATCH_5_CONFIGS],\n",
    "    \"total_questions\": len(items),\n",
    "    \"total_inferences\": len(BATCH_5_CONFIGS) * len(items),\n",
    "    \"llm_model\": llm_config.model,\n",
    "    \"max_iterations\": settings.max_iterations,\n",
    "}\n",
    "\n",
    "with open(batch_results_dir / \"batch_5_metadata.json\", \"w\") as f:\n",
    "    json.dump(batch_metadata, f, indent=2)\n",
    "\n",
    "print(\"Batch 5 completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Consolidate All Results\n",
    "\n",
    "After all 5 batches are completed, run the cells below to merge results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate all batch results\n",
    "print(\"=\"*60)\n",
    "print(\"CONSOLIDATING ALL BATCH RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_dir = project_root / \"results_v2\"\n",
    "all_summaries = []\n",
    "all_results = {}\n",
    "\n",
    "for batch_num in range(1, 6):\n",
    "    batch_dir = results_dir / f\"batch_{batch_num}\"\n",
    "    summary_path = batch_dir / f\"batch_{batch_num}_summary.csv\"\n",
    "    \n",
    "    if summary_path.exists():\n",
    "        df = pd.read_csv(summary_path)\n",
    "        df['Batch'] = batch_num\n",
    "        all_summaries.append(df)\n",
    "        print(f\"Loaded batch {batch_num}: {len(df)} configurations\")\n",
    "        \n",
    "        # Load detailed results\n",
    "        for config_name in df['Configuration'].values:\n",
    "            config_dir = batch_dir / config_name\n",
    "            csv_path = config_dir / \"agentic_results.csv\"\n",
    "            if csv_path.exists():\n",
    "                all_results[config_name] = pd.read_csv(csv_path)\n",
    "    else:\n",
    "        print(f\"Batch {batch_num} not found (run batch-{batch_num}_01 notebook first)\")\n",
    "\n",
    "if all_summaries:\n",
    "    df_all_summary = pd.concat(all_summaries, ignore_index=True)\n",
    "    print(f\"\\nTotal configurations: {len(df_all_summary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display consolidated summary\n",
    "if 'df_all_summary' in dir() and len(df_all_summary) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONSOLIDATED RESULTS - ALL 9 CONFIGURATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    display(df_all_summary[[\n",
    "        'Configuration', 'Success Rate (%)', 'Pass@1 Rate (%)', \n",
    "        'KG Valid Rate (%)', 'Avg Iterations'\n",
    "    ]].sort_values('Pass@1 Rate (%)', ascending=False))\n",
    "    \n",
    "    # Save consolidated summary\n",
    "    df_all_summary.to_csv(results_dir / \"consolidated_summary.csv\", index=False)\n",
    "    print(f\"\\nConsolidated summary saved to: {results_dir / 'consolidated_summary.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment summary JSON\n",
    "if 'df_all_summary' in dir() and len(df_all_summary) == 9:\n",
    "    experiment_summary = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"llm_provider\": \"groq\",\n",
    "        \"llm_model\": llm_config.model,\n",
    "        \"total_questions\": len(items),\n",
    "        \"total_configurations\": 9,\n",
    "        \"total_inferences\": 9 * len(items),\n",
    "        \"max_iterations\": settings.max_iterations,\n",
    "        \"configurations\": {}\n",
    "    }\n",
    "    \n",
    "    for _, row in df_all_summary.iterrows():\n",
    "        config_name = row['Configuration']\n",
    "        experiment_summary[\"configurations\"][config_name] = {\n",
    "            \"success_rate\": row['Success Rate (%)'],\n",
    "            \"pass_at_1_rate\": row['Pass@1 Rate (%)'],\n",
    "            \"kg_valid_rate\": row['KG Valid Rate (%)'],\n",
    "            \"avg_iterations\": row['Avg Iterations'],\n",
    "        }\n",
    "    \n",
    "    with open(results_dir / \"experiment_summary.json\", \"w\") as f:\n",
    "        json.dump(experiment_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"Experiment summary saved to: {results_dir / 'experiment_summary.json'}\")\n",
    "    print(\"\\nAll experiments completed! Proceed to 02_evaluation_metrics.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
