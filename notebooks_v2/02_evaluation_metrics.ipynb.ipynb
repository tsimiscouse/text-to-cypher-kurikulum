{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics Analysis (Groq - Qwen3-32B)\n",
    "\n",
    "Notebook ini menganalisis metrik evaluasi dari hasil agentic inference.\n",
    "\n",
    "## Prerequisites\n",
    "- All 5 batches completed (batch-1 to batch-5)\n",
    "- Consolidated results available in `results_v2/`\n",
    "\n",
    "## Metrik yang dianalisis:\n",
    "- **String Metrics**: BLEU, Rouge-L, Jaro-Winkler, Jaccard Cypher\n",
    "- **Output Metrics**: Pass@1, Jaccard Output\n",
    "- **Composite Metrics**: LLMetric-Q, LLMetric\n",
    "- **Agentic Metrics**: Iterations, Recovery Rate, First Attempt Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "os.chdir(project_root)\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Consolidated Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment summary\n",
    "results_dir = project_root / \"results_v2\"\n",
    "summary_path = results_dir / \"experiment_summary.json\"\n",
    "\n",
    "if summary_path.exists():\n",
    "    with open(summary_path, \"r\") as f:\n",
    "        summary = json.load(f)\n",
    "    print(f\"Loaded results from: {summary['timestamp']}\")\n",
    "    print(f\"LLM Provider: {summary.get('llm_provider', 'groq')}\")\n",
    "    print(f\"LLM Model: {summary.get('llm_model', 'qwen3-32b')}\")\n",
    "    print(f\"Configurations: {list(summary['configurations'].keys())}\")\n",
    "else:\n",
    "    print(\"No consolidated results found.\")\n",
    "    print(\"Please run all 5 batch notebooks first, then consolidate in batch-5.\")\n",
    "    summary = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load detailed results for each configuration\n",
    "detailed_results = {}\n",
    "\n",
    "if summary:\n",
    "    for batch_num in range(1, 6):\n",
    "        batch_dir = results_dir / f\"batch_{batch_num}\"\n",
    "        if batch_dir.exists():\n",
    "            for config_dir in batch_dir.iterdir():\n",
    "                if config_dir.is_dir() and config_dir.name != \"checkpoints\":\n",
    "                    csv_path = config_dir / \"agentic_results.csv\"\n",
    "                    if csv_path.exists():\n",
    "                        detailed_results[config_dir.name] = pd.read_csv(csv_path)\n",
    "                        print(f\"Loaded {len(detailed_results[config_dir.name])} results for {config_dir.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load consolidated summary CSV\n",
    "consolidated_path = results_dir / \"consolidated_summary.csv\"\n",
    "\n",
    "if consolidated_path.exists():\n",
    "    df_summary = pd.read_csv(consolidated_path)\n",
    "    print(f\"\\nLoaded consolidated summary: {len(df_summary)} configurations\")\n",
    "    display(df_summary)\n",
    "else:\n",
    "    print(\"Consolidated summary not found. Creating from batch summaries...\")\n",
    "    \n",
    "    all_summaries = []\n",
    "    for batch_num in range(1, 6):\n",
    "        batch_summary = results_dir / f\"batch_{batch_num}\" / f\"batch_{batch_num}_summary.csv\"\n",
    "        if batch_summary.exists():\n",
    "            df = pd.read_csv(batch_summary)\n",
    "            all_summaries.append(df)\n",
    "    \n",
    "    if all_summaries:\n",
    "        df_summary = pd.concat(all_summaries, ignore_index=True)\n",
    "        display(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate Additional Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import metrics calculators\n",
    "from metrics.string_metrics import calculate_bleu, calculate_rouge_l, calculate_jaro_winkler\n",
    "from metrics.output_metrics import calculate_jaccard_output\n",
    "from metrics.composite_metrics import calculate_llmetric_q, calculate_llmetric\n",
    "\n",
    "# Calculate detailed metrics for each configuration\n",
    "metrics_data = []\n",
    "\n",
    "for config_name, df in detailed_results.items():\n",
    "    config_metrics = {\n",
    "        \"Configuration\": config_name,\n",
    "        \"Total\": len(df),\n",
    "    }\n",
    "    \n",
    "    # Basic rates\n",
    "    config_metrics[\"Pass@1 (%)\"] = round(df['pass_at_1'].mean() * 100, 2) if 'pass_at_1' in df else 0\n",
    "    config_metrics[\"Success (%)\"] = round(df['success'].mean() * 100, 2)\n",
    "    config_metrics[\"KG Valid (%)\"] = round(df['kg_valid'].mean() * 100, 2) if 'kg_valid' in df else 0\n",
    "    \n",
    "    # Iterations\n",
    "    config_metrics[\"Avg Iterations\"] = round(df['total_iterations'].mean(), 2)\n",
    "    \n",
    "    # Calculate string metrics for each row\n",
    "    bleu_scores = []\n",
    "    rouge_scores = []\n",
    "    jaro_scores = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        if pd.notna(row['final_query']) and pd.notna(row['ground_truth']):\n",
    "            bleu_scores.append(calculate_bleu(row['ground_truth'], row['final_query']))\n",
    "            rouge_scores.append(calculate_rouge_l(row['ground_truth'], row['final_query']))\n",
    "            jaro_scores.append(calculate_jaro_winkler(row['ground_truth'], row['final_query']))\n",
    "    \n",
    "    config_metrics[\"Avg BLEU\"] = round(np.mean(bleu_scores), 4) if bleu_scores else 0\n",
    "    config_metrics[\"Avg Rouge-L\"] = round(np.mean(rouge_scores), 4) if rouge_scores else 0\n",
    "    config_metrics[\"Avg Jaro-Winkler\"] = round(np.mean(jaro_scores), 4) if jaro_scores else 0\n",
    "    \n",
    "    # Calculate LLMetric-Q for each row\n",
    "    llmetric_q_scores = []\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.notna(row['final_query']) and pd.notna(row['ground_truth']):\n",
    "            llm_q = calculate_llmetric_q(\n",
    "                ground_truth=row['ground_truth'],\n",
    "                generated=row['final_query'],\n",
    "                pass_at_1=row.get('pass_at_1', False)\n",
    "            )\n",
    "            llmetric_q_scores.append(llm_q)\n",
    "    \n",
    "    config_metrics[\"Avg LLMetric-Q\"] = round(np.mean(llmetric_q_scores), 2) if llmetric_q_scores else 0\n",
    "    \n",
    "    # Calculate LLMetric (configuration level)\n",
    "    config_metrics[\"LLMetric\"] = round(calculate_llmetric(\n",
    "        pass_at_1_rate=config_metrics[\"Pass@1 (%)\"] / 100,\n",
    "        kg_valid_rate=config_metrics[\"KG Valid (%)\"] / 100,\n",
    "        avg_llmetric_q=config_metrics[\"Avg LLMetric-Q\"]\n",
    "    ), 2)\n",
    "    \n",
    "    # Agentic metrics\n",
    "    first_attempt = df[df['total_iterations'] == 1]['success'].sum()\n",
    "    recovered = df[(df['total_iterations'] > 1) & (df['success'] == True)].shape[0]\n",
    "    initially_failed = df[df['total_iterations'] > 1].shape[0]\n",
    "    \n",
    "    config_metrics[\"First Attempt Success (%)\"] = round(first_attempt / len(df) * 100, 2)\n",
    "    config_metrics[\"Recovery Rate (%)\"] = round(recovered / initially_failed * 100, 2) if initially_failed > 0 else 0\n",
    "    \n",
    "    metrics_data.append(config_metrics)\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_data)\n",
    "print(\"\\nDetailed Metrics:\")\n",
    "display(df_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass@1 Rate by Configuration\n",
    "if len(df_metrics) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    configs = df_metrics['Configuration'].tolist()\n",
    "    pass_rates = df_metrics['Pass@1 (%)'].tolist()\n",
    "    \n",
    "    colors = plt.cm.Blues(np.linspace(0.4, 0.8, len(configs)))\n",
    "    bars = ax.bar(configs, pass_rates, color=colors)\n",
    "    \n",
    "    ax.set_xlabel('Configuration', fontsize=12)\n",
    "    ax.set_ylabel('Pass@1 Rate (%)', fontsize=12)\n",
    "    ax.set_title('Pass@1 Rate by Configuration (Groq - Qwen3-32B)', fontsize=14)\n",
    "    ax.set_xticklabels(configs, rotation=45, ha='right')\n",
    "    \n",
    "    for bar, val in zip(bars, pass_rates):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{val:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'pass_at_1_by_config.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMetric Comparison\n",
    "if len(df_metrics) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # LLMetric-Q\n",
    "    axes[0].bar(df_metrics['Configuration'], df_metrics['Avg LLMetric-Q'], color='coral')\n",
    "    axes[0].set_xlabel('Configuration')\n",
    "    axes[0].set_ylabel('LLMetric-Q Score')\n",
    "    axes[0].set_title('Average LLMetric-Q by Configuration')\n",
    "    axes[0].set_xticklabels(df_metrics['Configuration'], rotation=45, ha='right')\n",
    "    \n",
    "    # LLMetric\n",
    "    axes[1].bar(df_metrics['Configuration'], df_metrics['LLMetric'], color='seagreen')\n",
    "    axes[1].set_xlabel('Configuration')\n",
    "    axes[1].set_ylabel('LLMetric Score')\n",
    "    axes[1].set_title('LLMetric by Configuration')\n",
    "    axes[1].set_xticklabels(df_metrics['Configuration'], rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'llmetric_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agentic Metrics\n",
    "if len(df_metrics) > 0:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Average Iterations\n",
    "    axes[0].bar(df_metrics['Configuration'], df_metrics['Avg Iterations'], color='mediumpurple')\n",
    "    axes[0].set_xlabel('Configuration')\n",
    "    axes[0].set_ylabel('Average Iterations')\n",
    "    axes[0].set_title('Average Iterations Needed')\n",
    "    axes[0].set_xticklabels(df_metrics['Configuration'], rotation=45, ha='right')\n",
    "    axes[0].axhline(y=1, color='red', linestyle='--', alpha=0.5, label='Single attempt')\n",
    "    \n",
    "    # Recovery Rate\n",
    "    axes[1].bar(df_metrics['Configuration'], df_metrics['Recovery Rate (%)'], color='darkorange')\n",
    "    axes[1].set_xlabel('Configuration')\n",
    "    axes[1].set_ylabel('Recovery Rate (%)')\n",
    "    axes[1].set_title('Recovery Rate (Initially Failed -> Success)')\n",
    "    axes[1].set_xticklabels(df_metrics['Configuration'], rotation=45, ha='right')\n",
    "    \n",
    "    # First Attempt Success\n",
    "    axes[2].bar(df_metrics['Configuration'], df_metrics['First Attempt Success (%)'], color='teal')\n",
    "    axes[2].set_xlabel('Configuration')\n",
    "    axes[2].set_ylabel('First Attempt Success (%)')\n",
    "    axes[2].set_title('First Attempt Success Rate')\n",
    "    axes[2].set_xticklabels(df_metrics['Configuration'], rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'agentic_metrics.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis by Prompt Type and Schema Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse configuration names\n",
    "def parse_config(config_name):\n",
    "    parts = config_name.split('_')\n",
    "    if len(parts) >= 2:\n",
    "        prompt = parts[0]\n",
    "        schema = '_'.join(parts[1:])\n",
    "        return prompt, schema\n",
    "    return config_name, 'Unknown'\n",
    "\n",
    "df_metrics[['Prompt', 'Schema']] = df_metrics['Configuration'].apply(\n",
    "    lambda x: pd.Series(parse_config(x))\n",
    ")\n",
    "\n",
    "# Aggregate by Prompt Type\n",
    "prompt_agg = df_metrics.groupby('Prompt').agg({\n",
    "    'Pass@1 (%)': 'mean',\n",
    "    'KG Valid (%)': 'mean',\n",
    "    'LLMetric': 'mean',\n",
    "    'Avg Iterations': 'mean',\n",
    "    'Recovery Rate (%)': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nAggregate by Prompt Type:\")\n",
    "display(prompt_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by Schema Format\n",
    "schema_agg = df_metrics.groupby('Schema').agg({\n",
    "    'Pass@1 (%)': 'mean',\n",
    "    'KG Valid (%)': 'mean',\n",
    "    'LLMetric': 'mean',\n",
    "    'Avg Iterations': 'mean',\n",
    "    'Recovery Rate (%)': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nAggregate by Schema Format:\")\n",
    "display(schema_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: Prompt vs Schema\n",
    "if len(df_metrics) >= 9:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # LLMetric Heatmap\n",
    "    pivot_llmetric = df_metrics.pivot(index='Prompt', columns='Schema', values='LLMetric')\n",
    "    sns.heatmap(pivot_llmetric, annot=True, fmt='.2f', cmap='YlGnBu', ax=axes[0])\n",
    "    axes[0].set_title('LLMetric: Prompt x Schema')\n",
    "    \n",
    "    # Pass@1 Heatmap\n",
    "    pivot_pass = df_metrics.pivot(index='Prompt', columns='Schema', values='Pass@1 (%)')\n",
    "    sns.heatmap(pivot_pass, annot=True, fmt='.1f', cmap='YlGnBu', ax=axes[1])\n",
    "    axes[1].set_title('Pass@1 Rate (%): Prompt x Schema')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(results_dir / 'heatmap_prompt_schema.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank configurations by LLMetric\n",
    "df_ranked = df_metrics.sort_values('LLMetric', ascending=False).reset_index(drop=True)\n",
    "df_ranked.index = df_ranked.index + 1\n",
    "df_ranked.index.name = 'Rank'\n",
    "\n",
    "print(\"\\nConfigurations Ranked by LLMetric:\")\n",
    "display(df_ranked[['Configuration', 'Pass@1 (%)', 'KG Valid (%)', 'LLMetric', 'Avg Iterations', 'Recovery Rate (%)']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export metrics summary\n",
    "export_path = results_dir / 'metrics_summary.csv'\n",
    "df_metrics.to_csv(export_path, index=False)\n",
    "print(f\"Exported metrics to: {export_path}\")\n",
    "\n",
    "# Export to Excel with multiple sheets\n",
    "try:\n",
    "    with pd.ExcelWriter(results_dir / 'metrics_summary.xlsx', engine='openpyxl') as writer:\n",
    "        df_metrics.to_excel(writer, sheet_name='Summary', index=False)\n",
    "        df_ranked.to_excel(writer, sheet_name='Ranked')\n",
    "        prompt_agg.to_excel(writer, sheet_name='By Prompt')\n",
    "        schema_agg.to_excel(writer, sheet_name='By Schema')\n",
    "    print(f\"Exported Excel to: {results_dir / 'metrics_summary.xlsx'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not export to Excel: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluation metrics analysis complete!\")\n",
    "print(\"Proceed to 03_comparative_analysis.ipynb for comparison with baseline.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
