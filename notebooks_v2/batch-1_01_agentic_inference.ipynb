{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Batch 1: Agentic Text2Cypher Inference (Day 1/5)\n\n**Configurations**: Zero-Shot_Full, Zero-Shot_Nodes+Paths\n\n**Model**: Qwen2.5-Coder-32B-Instruct via OpenRouter API\n\n**Questions**: 52 × 2 = 104 inferences\n\n---\n\n## Metrics Used (Formal Naming from Established Research)\n\n| Category | Metric | Source |\n|----------|--------|--------|\n| First Attempt | Pass@1, KG Valid@1 | HumanEval/kg-axel |\n| After Refinement | Pass@k, KG Valid@k | HumanEval/kg-axel |\n| Improvement | Refinement Gain, Recovery Rate | Self-Refine (Madaan, 2023) |\n\n---\n\n## Rate Limit Strategy\n- OpenRouter API quota management\n- Estimated tokens per inference: ~5000 (with retries)\n- This batch: 104 inferences"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/tsimiscouse/Docs/Sarjana/Skripsi/kg-luthfi/agentic\n",
      "Working directory: /Users/tsimiscouse/Docs/Sarjana/Skripsi/kg-luthfi/agentic\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set project root\n",
    "project_root = Path.cwd().parent\n",
    "os.chdir(project_root)\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from config.settings import Settings\n",
    "from config.llm_config import LLMConfig\n",
    "from data.ground_truth_loader import GroundTruthLoader\n",
    "from prompts.prompt_manager import PromptManager, PromptType, SchemaFormat\n",
    "from experiment.batch_processor import BatchProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Provider: openrouter\n",
      "Model: qwen/qwen-2.5-coder-32b-instruct\n",
      "Max Iterations: 3\n"
     ]
    }
   ],
   "source": [
    "# Initialize settings\n",
    "settings = Settings()\n",
    "llm_config = LLMConfig()\n",
    "\n",
    "print(f\"LLM Provider: {llm_config.provider}\")\n",
    "print(f\"Model: {llm_config.model}\")\n",
    "print(f\"Max Iterations: {settings.max_iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key validated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Validate API Key\n",
    "try:\n",
    "    llm_config.validate()\n",
    "    print(\"API Key validated successfully!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 52 questions\n"
     ]
    }
   ],
   "source": [
    "# Load ground truth data\n",
    "loader = GroundTruthLoader()\n",
    "items = loader.load()\n",
    "print(f\"Loaded {len(items)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 1 Configurations\n",
    "\n",
    "| # | Prompt Type | Schema Format | Config Name |\n",
    "|---|-------------|---------------|-------------|\n",
    "| 1 | Zero-Shot | Full Schema | Zero-Shot_Full |\n",
    "| 2 | Zero-Shot | Nodes+Paths | Zero-Shot_Nodes+Paths |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: 2 configurations\n",
      "Total inferences: 104 = 2 × 52\n"
     ]
    }
   ],
   "source": [
    "# Define Batch 1 configurations\n",
    "BATCH_1_CONFIGS = [\n",
    "    {\"prompt\": PromptType.ZERO_SHOT, \"schema\": SchemaFormat.FULL_SCHEMA},\n",
    "    {\"prompt\": PromptType.ZERO_SHOT, \"schema\": SchemaFormat.NODES_PATHS},\n",
    "]\n",
    "\n",
    "print(f\"Batch 1: {len(BATCH_1_CONFIGS)} configurations\")\n",
    "print(f\"Total inferences: {len(BATCH_1_CONFIGS) * len(items)} = {len(BATCH_1_CONFIGS)} × {len(items)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: /Users/tsimiscouse/Docs/Sarjana/Skripsi/kg-luthfi/agentic/results_v2/batch_1\n"
     ]
    }
   ],
   "source": [
    "# Setup results directory\n",
    "results_dir = project_root / \"results_v2\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "batch_results_dir = results_dir / \"batch_1\"\n",
    "batch_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Results will be saved to: {batch_results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize batch processor\n",
    "processor = BatchProcessor(\n",
    "    settings=settings,\n",
    "    llm_config=llm_config,\n",
    "    checkpoint_dir=str(batch_results_dir / \"checkpoints\")\n",
    ")\n",
    "\n",
    "prompt_manager = PromptManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Batch 1 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run experiments for Batch 1\nbatch_1_results = {}\n\nfor i, config in enumerate(BATCH_1_CONFIGS):\n    config_name = prompt_manager.get_configuration_name(config[\"prompt\"], config[\"schema\"])\n    print(f\"\\n{'='*60}\")\n    print(f\"Configuration {i+1}/{len(BATCH_1_CONFIGS)}: {config_name}\")\n    print(f\"{'='*60}\")\n    \n    # Create config directory\n    config_dir = batch_results_dir / config_name\n    config_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Process batch\n    try:\n        results = processor.process_batch(\n            items=items,\n            prompt_type=config[\"prompt\"],\n            schema_format=config[\"schema\"],\n            batch_id=f\"batch1_{config_name}\",\n            resume=True\n        )\n        \n        batch_1_results[config_name] = results\n        \n        # Save results\n        results_data = [state.to_dict() for state in results]\n        \n        with open(config_dir / \"agentic_results.json\", \"w\") as f:\n            json.dump(results_data, f, indent=2, default=str)\n        \n        # Save as CSV with all formal metrics\n        # Using formal naming from established research:\n        # - Pass@1, Pass@k: HumanEval (OpenAI, 2021)\n        # - KG Valid: kg-axel baseline\n        # - Refinement Gain, Recovery Rate: Self-Refine (Madaan, 2023)\n        df = pd.DataFrame([{\n            \"question_id\": s.question_id,\n            \"question\": s.question,\n            \"ground_truth\": s.ground_truth_query,\n            \"final_query\": s.final_query,\n            # Pass@1 Metrics (First Attempt - Baseline Comparable)\n            \"pass_at_1\": s.pass_at_1,\n            \"kg_valid_at_1\": s.kg_valid_at_1,\n            # Pass@k Metrics (Final - After Refinement)\n            \"pass_at_k\": s.pass_at_k,\n            \"execution_accuracy\": s.execution_accuracy,\n            \"kg_valid_at_k\": s.kg_valid_at_k,\n            # Self-Refine Metrics (Improvement)\n            \"refinement_gain\": s.refinement_gain,\n            \"kg_valid_refinement_gain\": s.kg_valid_refinement_gain,\n            \"was_recovered\": s.was_recovered,\n            \"was_kg_recovered\": s.was_kg_recovered,\n            # Iteration info\n            \"total_iterations\": s.total_iterations,\n            # Backward compatibility\n            \"success\": s.success,\n            \"kg_valid\": s.kg_valid,\n        } for s in results])\n        df.to_csv(config_dir / \"agentic_results.csv\", index=False)\n        \n        # Quick stats using formal metric names\n        total = len(results)\n        pass_at_1_rate = sum(1 for s in results if s.pass_at_1) / total * 100\n        kg_valid_at_1_rate = sum(1 for s in results if s.kg_valid_at_1) / total * 100\n        pass_at_k_rate = sum(1 for s in results if s.pass_at_k) / total * 100\n        kg_valid_at_k_rate = sum(1 for s in results if s.kg_valid_at_k) / total * 100\n        refinement_gain = pass_at_k_rate - pass_at_1_rate\n        recovered_count = sum(1 for s in results if s.was_recovered)\n        initially_failed = total - sum(1 for s in results if s.pass_at_1)\n        recovery_rate = (recovered_count / initially_failed * 100) if initially_failed > 0 else 0\n        avg_iterations = sum(s.total_iterations for s in results) / total\n        \n        print(f\"\\n{config_name} Results (Formal Metrics):\")\n        print(f\"  --- Pass@1 (First Attempt - Baseline Comparable) ---\")\n        print(f\"  Pass@1 Rate: {pass_at_1_rate:.1f}%\")\n        print(f\"  KG Valid@1 Rate: {kg_valid_at_1_rate:.1f}%\")\n        print(f\"  --- Pass@k (After Refinement, k={settings.max_iterations}) ---\")\n        print(f\"  Pass@k Rate: {pass_at_k_rate:.1f}%\")\n        print(f\"  KG Valid@k Rate: {kg_valid_at_k_rate:.1f}%\")\n        print(f\"  --- Self-Refine Metrics ---\")\n        print(f\"  Refinement Gain: {refinement_gain:+.1f} pp\")\n        print(f\"  Recovery Rate: {recovery_rate:.1f}%\")\n        print(f\"  Avg Iterations: {avg_iterations:.2f}\")\n        \n    except Exception as e:\n        print(f\"Error processing {config_name}: {e}\")\n        import traceback\n        traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Batch 1 Summary\n\n### Formal Metric Names (Based on Established Research)\n\n| Metric | Source | Description |\n|--------|--------|-------------|\n| **Pass@1** | HumanEval (OpenAI, 2021) | First attempt success rate |\n| **Pass@k** | HumanEval (OpenAI, 2021) | Success within k iterations |\n| **KG Valid@1** | kg-axel (Baseline) | First attempt KG validity (syntax+schema+properties) |\n| **KG Valid@k** | kg-axel (Baseline) | Final KG validity after refinement |\n| **Refinement Gain** | Self-Refine (Madaan, 2023) | Pass@k - Pass@1 (absolute improvement) |\n| **Recovery Rate** | Self-Refine (Madaan, 2023) | % of initial failures that were recovered |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate batch summary with formal metric names\n# Using naming from established research:\n# - Pass@1, Pass@k: HumanEval (OpenAI, Kulal et al. 2019)\n# - KG Valid: kg-axel baseline research\n# - Refinement Gain, Recovery Rate: Self-Refine (Madaan et al., 2023)\n\nif batch_1_results:\n    summary_rows = []\n    \n    for config_name, results in batch_1_results.items():\n        total = len(results)\n        \n        # Pass@1 Metrics (First Attempt - Baseline Comparable)\n        pass_at_1_count = sum(1 for s in results if s.pass_at_1)\n        kg_valid_at_1_count = sum(1 for s in results if s.kg_valid_at_1)\n        \n        # Pass@k Metrics (After Refinement)\n        pass_at_k_count = sum(1 for s in results if s.pass_at_k)\n        kg_valid_at_k_count = sum(1 for s in results if s.kg_valid_at_k)\n        \n        # Self-Refine Metrics\n        recovered_count = sum(1 for s in results if s.was_recovered)\n        initially_failed = total - pass_at_1_count\n        recovery_rate = (recovered_count / initially_failed * 100) if initially_failed > 0 else 0\n        \n        kg_recovered_count = sum(1 for s in results if s.was_kg_recovered)\n        kg_initially_invalid = total - kg_valid_at_1_count\n        kg_recovery_rate = (kg_recovered_count / kg_initially_invalid * 100) if kg_initially_invalid > 0 else 0\n        \n        summary_rows.append({\n            \"Configuration\": config_name,\n            \"N\": total,\n            # Pass@1 (Baseline Comparable)\n            \"Pass@1 (%)\": round(pass_at_1_count / total * 100, 2),\n            \"KG Valid@1 (%)\": round(kg_valid_at_1_count / total * 100, 2),\n            # Pass@k (After Refinement)\n            \"Pass@k (%)\": round(pass_at_k_count / total * 100, 2),\n            \"KG Valid@k (%)\": round(kg_valid_at_k_count / total * 100, 2),\n            # Self-Refine Metrics\n            \"Refine Gain (pp)\": round(pass_at_k_count / total * 100 - pass_at_1_count / total * 100, 2),\n            \"Recovery (%)\": round(recovery_rate, 2),\n            \"KG Recovery (%)\": round(kg_recovery_rate, 2),\n            # Iteration Stats\n            \"Avg Iter\": round(sum(s.total_iterations for s in results) / total, 2),\n        })\n    \n    df_summary = pd.DataFrame(summary_rows)\n    display(df_summary)\n    \n    # Save summary\n    df_summary.to_csv(batch_results_dir / \"batch_1_summary.csv\", index=False)\n    print(f\"\\nSummary saved to: {batch_results_dir / 'batch_1_summary.csv'}\")\n    \n    # Metric explanation\n    print(\"\\n\" + \"=\"*70)\n    print(\"METRIC NAMING (Based on Established Research)\")\n    print(\"=\"*70)\n    print(\"Pass@1, Pass@k    : HumanEval Benchmark (OpenAI, Kulal et al. 2019)\")\n    print(\"KG Valid@1/k      : kg-axel baseline research (comparable metric)\")\n    print(\"Refinement Gain   : Self-Refine (Madaan et al., 2023) - Pass@k - Pass@1\")\n    print(\"Recovery Rate     : Self-Refine - % of initial failures that were fixed\")\n    print(\"=\"*70)\n    print(\"\\nNOTE: 'KG Valid@1 (%)' should match kg-axel's 'KG Valid Query Rate'\")\n    print(\"      for baseline comparison.\")"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 completed!\n",
      "Continue with batch-2_01_agentic_inference.ipynb tomorrow.\n"
     ]
    }
   ],
   "source": [
    "# Save batch metadata\n",
    "batch_metadata = {\n",
    "    \"batch_number\": 1,\n",
    "    \"date\": datetime.now().isoformat(),\n",
    "    \"configurations\": [prompt_manager.get_configuration_name(c[\"prompt\"], c[\"schema\"]) for c in BATCH_1_CONFIGS],\n",
    "    \"total_questions\": len(items),\n",
    "    \"total_inferences\": len(BATCH_1_CONFIGS) * len(items),\n",
    "    \"llm_model\": llm_config.model,\n",
    "    \"max_iterations\": settings.max_iterations,\n",
    "}\n",
    "\n",
    "with open(batch_results_dir / \"batch_1_metadata.json\", \"w\") as f:\n",
    "    json.dump(batch_metadata, f, indent=2)\n",
    "\n",
    "print(\"Batch 1 completed!\")\n",
    "print(f\"Continue with batch-2_01_agentic_inference.ipynb tomorrow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "processor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}