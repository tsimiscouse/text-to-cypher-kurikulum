{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch 1: Agentic Text2Cypher Inference (Day 1/5)\n",
    "\n",
    "**Configurations**: Zero-Shot_Full, Zero-Shot_Nodes+Paths\n",
    "\n",
    "**Model**: Qwen3-32B via Groq API\n",
    "\n",
    "**Questions**: 52 × 2 = 104 inferences\n",
    "\n",
    "---\n",
    "\n",
    "## Rate Limit Strategy\n",
    "- Groq API: 500K tokens/day\n",
    "- Estimated tokens per inference: ~5000 (with retries)\n",
    "- Safe limit: ~100 inferences/day\n",
    "- This batch: 104 inferences (should fit within limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/tsimiscouse/Docs/Sarjana/Skripsi/kg-luthfi/agentic\n",
      "Working directory: /Users/tsimiscouse/Docs/Sarjana/Skripsi/kg-luthfi/agentic\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set project root\n",
    "project_root = Path.cwd().parent\n",
    "os.chdir(project_root)\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from config.settings import Settings\n",
    "from config.llm_config import LLMConfig\n",
    "from data.ground_truth_loader import GroundTruthLoader\n",
    "from prompts.prompt_manager import PromptManager, PromptType, SchemaFormat\n",
    "from experiment.batch_processor import BatchProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Provider: openrouter\n",
      "Model: qwen/qwen-2.5-coder-32b-instruct\n",
      "Max Iterations: 3\n"
     ]
    }
   ],
   "source": [
    "# Initialize settings\n",
    "settings = Settings()\n",
    "llm_config = LLMConfig()\n",
    "\n",
    "print(f\"LLM Provider: {llm_config.provider}\")\n",
    "print(f\"Model: {llm_config.model}\")\n",
    "print(f\"Max Iterations: {settings.max_iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key validated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Validate API Key\n",
    "try:\n",
    "    llm_config.validate()\n",
    "    print(\"API Key validated successfully!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 52 questions\n"
     ]
    }
   ],
   "source": [
    "# Load ground truth data\n",
    "loader = GroundTruthLoader()\n",
    "items = loader.load()\n",
    "print(f\"Loaded {len(items)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 1 Configurations\n",
    "\n",
    "| # | Prompt Type | Schema Format | Config Name |\n",
    "|---|-------------|---------------|-------------|\n",
    "| 1 | Zero-Shot | Full Schema | Zero-Shot_Full |\n",
    "| 2 | Zero-Shot | Nodes+Paths | Zero-Shot_Nodes+Paths |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: 2 configurations\n",
      "Total inferences: 104 = 2 × 52\n"
     ]
    }
   ],
   "source": [
    "# Define Batch 1 configurations\n",
    "BATCH_1_CONFIGS = [\n",
    "    {\"prompt\": PromptType.ZERO_SHOT, \"schema\": SchemaFormat.FULL_SCHEMA},\n",
    "    {\"prompt\": PromptType.ZERO_SHOT, \"schema\": SchemaFormat.NODES_PATHS},\n",
    "]\n",
    "\n",
    "print(f\"Batch 1: {len(BATCH_1_CONFIGS)} configurations\")\n",
    "print(f\"Total inferences: {len(BATCH_1_CONFIGS) * len(items)} = {len(BATCH_1_CONFIGS)} × {len(items)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: /Users/tsimiscouse/Docs/Sarjana/Skripsi/kg-luthfi/agentic/results_v2/batch_1\n"
     ]
    }
   ],
   "source": [
    "# Setup results directory\n",
    "results_dir = project_root / \"results_v2\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "batch_results_dir = results_dir / \"batch_1\"\n",
    "batch_results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Results will be saved to: {batch_results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize batch processor\n",
    "processor = BatchProcessor(\n",
    "    settings=settings,\n",
    "    llm_config=llm_config,\n",
    "    checkpoint_dir=str(batch_results_dir / \"checkpoints\")\n",
    ")\n",
    "\n",
    "prompt_manager = PromptManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Batch 1 Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Configuration 1/2: Zero-Shot_Full\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Run experiments for Batch 1\n",
    "batch_1_results = {}\n",
    "\n",
    "for i, config in enumerate(BATCH_1_CONFIGS):\n",
    "    config_name = prompt_manager.get_configuration_name(config[\"prompt\"], config[\"schema\"])\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Configuration {i+1}/{len(BATCH_1_CONFIGS)}: {config_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create config directory\n",
    "    config_dir = batch_results_dir / config_name\n",
    "    config_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Process batch\n",
    "    try:\n",
    "        results = processor.process_batch(\n",
    "            items=items,\n",
    "            prompt_type=config[\"prompt\"],\n",
    "            schema_format=config[\"schema\"],\n",
    "            batch_id=f\"batch1_{config_name}\",\n",
    "            resume=True\n",
    "        )\n",
    "        \n",
    "        batch_1_results[config_name] = results\n",
    "        \n",
    "        # Save results\n",
    "        results_data = [state.to_dict() for state in results]\n",
    "        \n",
    "        with open(config_dir / \"agentic_results.json\", \"w\") as f:\n",
    "            json.dump(results_data, f, indent=2, default=str)\n",
    "        \n",
    "        # Save as CSV\n",
    "        df = pd.DataFrame([{\n",
    "            \"question_id\": s.question_id,\n",
    "            \"question\": s.question,\n",
    "            \"ground_truth\": s.ground_truth_query,\n",
    "            \"final_query\": s.final_query,\n",
    "            \"success\": s.success,\n",
    "            \"pass_at_1\": s.first_attempt_success,\n",
    "            \"total_iterations\": s.total_iterations,\n",
    "            \"kg_valid\": s.kg_valid,\n",
    "        } for s in results])\n",
    "        df.to_csv(config_dir / \"agentic_results.csv\", index=False)\n",
    "        \n",
    "        # Quick stats\n",
    "        success_rate = sum(1 for s in results if s.success) / len(results) * 100\n",
    "        pass_at_1 = sum(1 for s in results if s.first_attempt_success) / len(results) * 100\n",
    "        avg_iterations = sum(s.total_iterations for s in results) / len(results)\n",
    "        \n",
    "        print(f\"\\n{config_name} Results:\")\n",
    "        print(f\"  Success Rate: {success_rate:.1f}%\")\n",
    "        print(f\"  Pass@1 Rate: {pass_at_1:.1f}%\")\n",
    "        print(f\"  Avg Iterations: {avg_iterations:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {config_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 1 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate batch summary\n",
    "if batch_1_results:\n",
    "    summary_rows = []\n",
    "    \n",
    "    for config_name, results in batch_1_results.items():\n",
    "        success_count = sum(1 for s in results if s.success)\n",
    "        pass_at_1_count = sum(1 for s in results if s.first_attempt_success)\n",
    "        kg_valid_count = sum(1 for s in results if s.kg_valid)\n",
    "        \n",
    "        summary_rows.append({\n",
    "            \"Configuration\": config_name,\n",
    "            \"Total\": len(results),\n",
    "            \"Success\": success_count,\n",
    "            \"Success Rate (%)\": round(success_count / len(results) * 100, 2),\n",
    "            \"Pass@1\": pass_at_1_count,\n",
    "            \"Pass@1 Rate (%)\": round(pass_at_1_count / len(results) * 100, 2),\n",
    "            \"KG Valid\": kg_valid_count,\n",
    "            \"KG Valid Rate (%)\": round(kg_valid_count / len(results) * 100, 2),\n",
    "            \"Avg Iterations\": round(sum(s.total_iterations for s in results) / len(results), 2),\n",
    "        })\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_rows)\n",
    "    display(df_summary)\n",
    "    \n",
    "    # Save summary\n",
    "    df_summary.to_csv(batch_results_dir / \"batch_1_summary.csv\", index=False)\n",
    "    print(f\"\\nSummary saved to: {batch_results_dir / 'batch_1_summary.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save batch metadata\n",
    "batch_metadata = {\n",
    "    \"batch_number\": 1,\n",
    "    \"date\": datetime.now().isoformat(),\n",
    "    \"configurations\": [prompt_manager.get_configuration_name(c[\"prompt\"], c[\"schema\"]) for c in BATCH_1_CONFIGS],\n",
    "    \"total_questions\": len(items),\n",
    "    \"total_inferences\": len(BATCH_1_CONFIGS) * len(items),\n",
    "    \"llm_model\": llm_config.model,\n",
    "    \"max_iterations\": settings.max_iterations,\n",
    "}\n",
    "\n",
    "with open(batch_results_dir / \"batch_1_metadata.json\", \"w\") as f:\n",
    "    json.dump(batch_metadata, f, indent=2)\n",
    "\n",
    "print(\"Batch 1 completed!\")\n",
    "print(f\"Continue with batch-2_01_agentic_inference.ipynb tomorrow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "processor.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
